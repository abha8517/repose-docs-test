[{"topic_url": "9.0.1.0/architecture/configuration.html", "title": "9.0.1.0 | Architecture | Configuration", "content": "\nAfter installing Repose, config files can be found in /etc/repose.\nAt a minimum, Repose needs a container.cfg.xml and system-model.cfg.xml to start.\n\nThe container.cfg.xml config file tells Repose where to find component artifacts and where to deploy them while running.\nSee Container for more details.\n\nThe main config file is system-model.cfg.xml.\nThis file configures the list of filters, the deployment layout, and the location of the origin service.\nOut of the box, Repose is setup to run as a single node listening on port 8080, to run with no filters and services enabled, and to point to rackspace.com on port 80 as the origin service.\nSee System Model for more details.\n\nMost filters and services have their own configuration file.\nSee Filters and Services for more details.\n\nRepose reads the config files when it starts up and when a file gets updated.\nThe configuration files are checked for updates every 15 seconds.\nThis is not configurable nor can it be manually triggered.\n\nWhen the config files are invalid, Repose will continue using the previous known good configuration, or if Repose was just initially starting up, then it will return a 503 until the config files are fixed.\n\nTo make configuration files more portable, environment variable substitution is supported.\nWith this feature, configuration files become semi-dynamic\u2009\u2014\u2009the same configuration files can be used to define differing behavior dependent on the environment Repose is running in.\n\nSubstituting an environment variable into a configuration file is easy!\nWherever you want the value of an environment variable in your configuration file, simply surround the name of the environment variable with output delimiters.\n\nUnder the hood, the JTwig templating engine is used.\nHowever, Repose does not use all of the JTwig default delimiters.\nThe following table shows the differences.\n\nApart from delimiters, the Repose template syntax matches the JTwig syntax.\n\nIn addition to simple substitution, JTwig provides a number of other well-documented features.\n\nWhen substituting an environment variable, Repose strictly requires that the named environment variable exists.\nIf not, Repose will throw an exception when processing the containing configuration file.\n\nUpdating an environment variable will not cause Repose to reload any configuration which references that environment variable.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/architecture/container.html", "title": "9.0.1.0 | Architecture | Container", "content": "\nThe container configuration is required to set up Repose.\nIt is used to tell the system where to look for component artifacts and where to deploy them.\n\nThe following example shows a basic configuration for the Container file.\n\nArtifacts are files containing resources which extend the functionality of Repose.\nTechnically, Repose artifacts are Enterprise Application Archives (EARs).\n\nFor example, as part of the package installation of Repose (either using a DEB or RPM package), a filter-bundle-x.x.x.x.ear artifact is placed in the default artifact directory.\nThe filter bundle artifact provides a number of filters that can be used in the filter chain, as well as descriptors of those filters.\nWithout this artifact, Repose would not know about the filters contained within, and therefore, would be not able to use those filters in the filter chain.\n\nThe deployment process is the process by which Repose organizes the content of Artifacts in preparation for utilization.\n\nTo be more specific, the deployment process is comprised of the following steps:\n\nSince each artifact is extracted to a directory named the hash of said artifact, deployment directory names are consistent and predictable.\nAs a result, there will be only one copy of the extracted contents of each artifact, and therefore, disk space usage should also be predictable.\n\nIf an artifact file exists on disk prior to the deployment process, the checksum of the artifact file will be compared to the checksum of the artifact to be deployed.\nIf the checksums match, deployment of the artifact in question will be skipped.\nIf the checksums do not match, the artifact will be deployed, overwriting the existing artifact file.\nTo prevent contention during the deployment process, a platform-dependent lock will be acquired on the artifact file.\nThe lock should prevent other processes from accessing the artifact file while it is being processed.\nUltimately, the checksum and locking strategy should ensure valid deployments while preventing unnecessary writes to disk and file contention when multiple Repose processes are running concurrently.\n\nThe auto-clean feature may be used to delete deployment directories when Repose shuts down.\nThis feature helps manage disk space usage, and remove the contents of artifacts which are no longer in use.\nOnly deployment directories created by the instance of Repose which is shutting down will be deleted.\n\nIf all of the follow conditions are met, Repose may delete deployment directories that are in use by other Repose processes:\n\nTo avoid this situation, it is highly recommended that the deployment-directory is not shared between more than one Repose instance if the auto-clean feature is used.\n\nIf the auto-clean feature is not being used, it should be safe to share the deployment-directory between multiple Repose instances.\n\nFiles in the deployment directory are only removed if the auto-clean feature is used, and even then, only when Repose shuts down.\nSince files in the deployment directory are made accessible to Repose at runtime (via the classpath), files in the deployment directory that are not managed by Repose could affect the behavior of Repose and could even result in errors.\nAs such, care should be taken to ensure that errant files do not end up in the deployment directory.\n\nSomething else to be aware of is that the deployment-directory and artifact-directory values should be fully qualified paths.\nThis is the case as the exact location of the launching JVM may not remain in the same location and this would effect relative paths.\nThe path should not start or end with a whitespace character as all leading and trailing whitespace is ignored in reading this value.\nAll white space in the middle of the path name will be preserved and should not be escaped in any way.\n\nThe following assume:\n\nThe interaction between the deprecated via attribute and the new via-header element are as follows:\n\nThe following further assumes the deprecated via attribute is not defined and the via-header element contains:\n\nThere are two steps to supply a logging configuration for Repose:\n\nIf a user-supplied logging configuration file is not found, Repose programmatically sets default log4j properties.\nThis default properties add a\nConsoleAppender\nto the ROOT logger.\nThe output will be formatted using a\nPatternLayout\nset to the pattern %d %-4r [%t] %-5p %c - %m%n.\nThe default log level is set to DEBUG.\n\nSSL/TLS Client Authentication is being used more and more for communications between different enclaves.\nThis addition to the SSL/TLS handshake involves the Client presenting credentials to the Server in the same manner as the Server does to the Client.\nIf the credentials presented by the Client are not trusted, then the Server will sever the connection just as the Client would have if the situation was reversed.\nSince a Client initiates contact with the Server, the Server\u2019s credentials are simply to validate it is who the Client was trying to contact.\nThis is accomplished through Certificate Authorities (CA) and the Trust Hierarchies built into the Public Key Infrastructure (PKI).\nEven though you can optionally add a particular Server\u2019s credentials directly into a Client so that it will implicitly trust a particular Server essentially bypassing the distributed trust mechanism in favor of a more direct one, this is the only way to build a relationship for a Client to a Server.\n\nTo require SSL/TLS Client Authentication, set the need-client-auth attribute to True.\nWith this setting enabled, only Clients that have a Public Key imported into the trust store referenced by the truststore-filename element will be allowed to connect.\nThe truststore is a\nJava Keystore\nthat can be created/updated using the command line tool named aptly enough,\nkeytool.\nBelow is an example of importing a Client certificate (client.crt) into a truststore (truststore.jks):\n\nThis will update the keystore if it exists or create a new one if it doesn\u2019t.\nThe tool will also prompt for a password.\nThe password will be used to access an existing file or set as the password on a new one.\n\nTo use the truststore created/updated in the example above, the following would need to be added/updated in the container.cfg.xml file:\n\nFor more details, see:\n\nRepose Valve is based on Jetty and uses its services for SSL/TLS termination.\nTo enable this feature you need to:\n\nKeystore information is located within the <ssl-configuration> element as shown in the following example.\n\nSince security is a constantly moving target, any recommended configuration would quickly become out of date.\nA risk assessment should always be performed by the appropriately qualified people for your organization.\nLinks to industry-standard references are provided in the SSL References section below.\n\nRepose supports whitelisting and blacklisting specific protocols and ciphers by exposing portions of the Jetty configuration via the container.cfg.xml file.\nYou can use this feature if a specific protocol or cipher has been compromised and you want to block its usage and harden your Repose instance.\nAll of Jetty\u2019s built-in default allowed protocols and ciphers are cleared and then the configured inclusion and exclusion lists are applied if an ssl-configuration is specified.\n\nWhen working with Includes / Excludes, it is important to know that Excludes will always win.\n\nProtocols and ciphers are configured using the same process.\nWe start with the list of all that are available on the host system.\nThis list is culled to contain only those that match the included RegEx statements.\nThen the list is further culled to remove any that match the excluded RegEx statements.\n\nIn the following example, the container configuration includes only the recommended TLS ciphers and protocols.\nThis in turn automatically excludes all SSL protocols and ciphers as they do not meet the inclusion criteria.\n\nYou need to specify your keystore in the container configuration just as you would in Jetty.\n\nCertain attacks (such as Logjam) leverage the weakness of \"small\" Diffie-Hellman (DH) keys.\nTo mitigate the risk of such attackers, users may either exclude vulnerable ciphers, or lengthen the DH keys used by Repose.\nInstructions for the former are above.\nFor the latter, note the following:\n\nDiffie-Hellman (DH) keys of sizes less than 1024 bits have been deprecated because of their insufficient strength.\nIn JDK 8, you can customize the ephemeral DH key size with the system property jdk.tls.ephemeralDHKeySize.\n\nIn other words, the Java option -Djdk.tls.ephemeralDHKeySize=2048 can be passed when starting Repose to force the use of longer DH keys.\n\nFor more details, see Customizing DH Keys.\n\nFor more information about cipher suites and which ones to dis/allow when setting up Repose, see the following references:\n\nThe list of available ciphers and protocols varies depending on the JVM.\nWe have added a command line option to Repose Valve to display the available and default enabled ciphers and protocols:\n\nThis will dump a list of the default enabled SSL/TLS parameters for the JVM you\u2019re using.\nAdditionally, it will list all available ciphers and protocols, should you wish to use one of those.\n\nThis mode should only be used during development testing.\nThese settings are NOT intended for a production environment.\n\nWhen running in insecure mode, Repose will accept all certificates from external services with which it communicates (e.g., authentication service, origin service).\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/architecture/filter-chain.html", "title": "9.0.1.0 | Architecture | Filter Chain", "content": "\nThe Repose filter chain is the work horse of the system, and is responsible for nearly all enrichment that Repose provides.\nIt\u2019s based on and uses the same contract as the JEE Filter Chain.\nAs the name implies it is a chain of filters that are followed in order.\nA request goes through them from the first, one after another, until the last is reached when the request which has possibly been mutated along the way is sent on to the origin service.\nWhen the response comes back from the origin service it works it\u2019s way back up the chain in the opposite order.\n\nThe Repose filter chain provides a number of enhancements over the standard filter chain contract that gives a lot of power and extra value.\n\nTypically a filter chain is static and unchanging.\nYour request will pass through every single filter everytime regardless of whether you want it to or not.\nThe Repose filter chain allows you to determine if a filter should be run while the request is processing.\n\nThis method of determining if a filter should be run is DEPRECATED and will be removed in Repose 10.\n\nYou can decide whether or not a filter should run based on the path of it\u2019s URI using a regular expression.\nHere\u2019s a simple example:\n\nIt\u2019s worth noting that if anything changes the request uri while processing the filter chain that will potentially impact whether a filter is run.\nOr in other words the filters to be run is not determined up front, but instead evaluated when the request hits a given filter in the chain.\n\nYou can configure whether or not a filter should run based on each of the following HTTP Request criterion:\n\nThis is further customizable through the use of the following boolean logic operators:\n\nThe filter chain is dynamic.\nIt is not determined at the start of the request, rather before each filter is executed the request is compared against the configured criteria.\nThis means that anything that modifies the request may effect whether a later filter will process or not.\nFor example, the Body Extractor to Header Filter could add a header based on the request body which is used to determine if a later filter executes.\n\nRefer to System Model Filter Activation Determination for further details about configuring this capability.\n\nSometimes you won\u2019t get the output you expect from the filter chain whether that\u2019s to your origin service or your customer that originated the request.\nIntrafilter logging will let you get a look into what effect each filter is having on both the request and response.\nAll you have to do is change this line in your logging configuration, from:\n\nto:\n\nThis will convert both the request and response into a simple JSON object that will be written out into the logs.\nAnd it will do this before each filter and the origin service allowing you to see how each filter mutated them.\n\nThis can be expensive in both time and disk space and is intended only as a debugging tool, not for full time use.\n\nFor more troubleshooting help see our guide.\n\nRepose automatically records metrics on how long each request spends within a filter.\nIf you wish to see these numbers or exfiltrate them to another tool look into the Metrics Service.\nThey are listed under org.openrepose.core.FilterProcessingTime.Delay with an entry for each filter by name.\n\nIf you are curious about the numbers for a specific request you can add the X-Trace-Request header to the request.\nSee Time Spent in Each Filter for more details.\n\nWe also start an OpenTracing span for each filter that is ran.\nIf you\u2019d like to get this data look into the OpenTracing Service.\n\nSometimes you\u2019d like a request to entirely bypass the filter chain entirely.\nThe most common use case for this is if you have some sort of monitoring setup to check the responsiveness of your origin service.\nWe support this with a simple regular expression specified at the top of the filter chain, like so:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/architecture/logging.html", "title": "9.0.1.0 | Architecture | Logging", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/architecture/packages.html", "title": "9.0.1.0 | Architecture | Packages", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\nProvides details about the packages Repose maintains for easy installation by our users.\n\nThis section outlines where Repose files live in the filesystem after being extracted from one of our packages.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/architecture/servlet-spec.html", "title": "9.0.1.0 | Architecture | Servlet Specification", "content": "\nThe Java Servlet Specification is a document that details the design and API of a Java-based system that can respond to requests.\nTo learn more about the specification itself, visit the link near the start of this paragraph.\n\nRepose is built on top of the servlet specification.\nHowever, Repose also constrains the specification in certain ways.\nFor example, Repose can only service HTTP requests.\n\nTo understand the architecture of Repose, it is vital that one understands the servlet specification.\nCurrently, Repose complies with version 3.0 of the specification.\n\nThe ReposeFilter class implements the Filter API and is the only filter in the base containers FilterChain.\nIt uses the ReposeFilterLoader class to load the FilterContext 's that are passed to the ReposeFilterChain.\n\nBefore calling into the Repose Filter Chain, it enhances the request from the client with a unique Transaction ID, if one is not already present, and starts a span with the OpenTracing Service.\nWhen the Repose Filter Chain completes, the OpenTracing span is closed, the Via header is updated on the response, and the Metrics Service is updated.\n\nThe ReposeFilterChain class implements the FilterChain API, and is responsible for the coordination between filters.\nThat coordination includes things like dynamic determination of whether or not a filter should process a request or response, debug logging, and metrics gathering.\nTo learn more about this component, visit the filter chain documentation page.\n\nThe ReposeRoutingServlet class implements the Servlet API and is registered with the servlet container.\nIt is responsible for routing the HTTP request from Repose to the origin service after the filter chain has finished processing the request.\n\nThis servlet uses the configuration information stored in the System Model to determine what destinations are available.\nRouting to an app on the same host but in another container or environment is easily configured in a generic way.\n\nThe preferred deployment strategy is to put a repose node as a side-car proxy on the same host as the origin service in a 1 to 1 fashion.\nIf Repose needs to horizontally scale at a different rate from the origin service, then putting an external load balancer as the destination is the safer option.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/architecture/system-model.html", "title": "9.0.1.0 | Architecture | System Model", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\nThe Repose system-model.cfg.xml is the main configuration file for Repose.\nIt outlines the deployment layout and behavior for Repose.\nRepose must be aware of this in order to configure itself and correctly coordinate routing if necessary.\nAdditionally, the system model lets Repose know where the other Repose nodes reside.\nUsing this information, Repose can coordinate its own clustering to share data among nodes that share common filters.\n\nThis section is only meant to explain certain quirks of the system model, and to provide a link to the comprehensive configuration schema.\nOther aspects of configuration will be explained through examples. See: Repose Deployment Scenarios\n\nSince port numbers below 1024 are privileged, Repose typically can not connect directly to them.\nThere are several ways to go about getting around this, but one of the most generally accepted ways is to execute the following commands with root privilege (e.g., sudo):\n\nStreaming a request body to the origin service is accomplished by using the chunked transfer encoding.\n\nIf the origin service does not support the chunked transfer-encoding (e.g., WSGI), it can be turned off easily.\nSimply set the chunked-encoding attribute to false.\n\nSetting the chunked-encoding attribute to false may cause Repose to read and temporarily buffer the request body content.\nThis will cause some performance degradation as request body is no longer always streamed through.\nIt is recommended that users leave this value as true unless their service does not support chunked encoding.\n\nIN PROGRESS\n\nIN PROGRESS\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/developer/docker.html", "title": "9.0.1.0 | Developer | Docker", "content": "\nThis is a collection of information cobbled together from several sources, but mostly from sifting through the Official Docker Documentation and then applying it to Repose.\n\nThe rest of this assumes you are already familiar with the contents of the Quick Start and Docker recipes.\nSince you made it here your hunger for Repose Docker knowledge must not have been satiated, so read on and enjoy.\n\nBasically you just need to:\n\nThe Repose build system expects your Docker Hub credentials to be available at build time.\nThey can be passed in as build properties or the easier way is to add them to your ~/.gradle/gradle.properties file:\n\nThen try to build a quick Repose Docker image from the root of the Repose project.\n\nThis will ultimately result in a message similar to:\n\nBuilding an image to run a specific version of Repose can be done by providing the desired version as a build property.\n\nOnly images for Repose v9.0.0.0 and above can be built this way.\nThe Dockerfile instructions used to build the images expect to install the core of Repose from the repose package.\nPrior to Repose v9.0.0.0, the repose-valve was used instead, but was renamed.\n\nIf you wish to tag an image yourself, an untagged image can be built.\n\nUsing the IMAGE_ID provided in the Gradle output (or via a docker images query), tag the new Repose Docker image.\n\nThe published images adhere to the expectations of Twelve-Factor App logging via streaming everything to STDOUT/STDERR and this is covered further in the Quick Start.\nSometimes though, it may be handy to enable the regular Repose Valve logging that is disabled in the published images.\n\nIf you are using a published image, then you can execute an interactive session on a running container and uncomment the two AppenderRef lines below in the logging configuration file to begin logging to the standard /var/log/repose/ location inside the container:\n\nIf you are building your own image based on one of the Repose Dockerfile's, then you can simply remove the lines below and the Repose image will log to the standard /var/log/repose/ location.\n\nThe /var/log/repose/ logging directory has also been modified in the Dockerfile to allow logging to be turned on and also support arbitrary user ID\u2019s like are used in the OpenShift environment.\n\nTo access the Repose log files, you can mount the directory as a Docker volume when starting the container using a command like the following:\n\nIf you didn\u2019t recognize it, this is almost identical to the command provided in Using a Repose Docker Image, but with the extra volume mount option.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/developer/functional.html", "title": "9.0.1.0 | Developer | Functional Testing", "content": "\nThe functional tests are categorized for easy division of labor when running the build.\nThe primary categorization is along package lines Core, Services, and Filters.\nIf a test needs further consideration because of unique needs we use the additional categories Bug, Intense, XmlParser, Smoke, and Recipe.\n\nWhile the build could be split strictly along package lines with the unique status categories applied on top, for ease of understanding and configuring the build it was decided to map packages onto categories as well.\nFor the same reasons it was decided to only allow one category per test, with a special status category taking precedence over a package category.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/developer/performance.html", "title": "9.0.1.0 | Developer | Performance Testing", "content": "\nPerformance testing is a method of testing various aspects of Repose in order to collect performance data.\nPerformance data can be used to determine the overhead of the aspects in question, both at a system resource level as well as at a response time level.\n\nTo performance test Repose, and collect the resulting data for analysis, the Framework is used.\n\nIn an attempt to provide the most meaningful data possible, the following test cases will be covered by independent tests:\n\nThe overall design of the automatic performance testing framework can be outlined succinctly by the following diagram:\n\nA code change in the Repose GitHub repository on the master branch will trigger a Jenkins job to run.\nThe Jenkins job will run the Ansible playbooks for all defined test cases.\nThe Ansible playbooks, in turn, will provision the necessary cloud resources and begin executing the Gatling simulations for each test case.\nAs Gatling runs, it will pipe the performance data it captures to InfluxDB.\nOnce Gatling execution completes, the cloud resources will be cleaned up, and the Ansible playbook will finish executing.\nGrafana will query InfluxDB to retrieve the performance data which is available in the Performance Tests dashboard.\n\nData will be transported from Gatling to InfluxDB using the Graphite line protocol.\nThe data being transferred will include numerous statistics calculated for a distinct time.\nTo get a better idea of the data in question, a sample has been provided which captures the number of request/response pairs sent at time 1492629621 as well as various statistics calculated from the amount of time taken to service those requests.\nNote that the sample data only captures a single second of the Gatling simulation.\nMuch more data of the same form would be expected for every second that the Gatling simulation ran.\n\nSystem data will be transported from Repose hosts to InfluxDB using Telegraf.\nMore specifically, Telegraf will collect system metrics like CPU usage, memory usage, and disk usage, and report those metrics to a UDP listener in InfluxDB.\nThese metrics will provide better insight into the system constraints of hosts running Repose.\n\nInfluxDB will process the data it receives through its Graphite listener port by applying configured templates.\nThese templates will translate the metric names from the Graphite line protocol into measurements, fields, tags, and values in the database.\n\nFor example, given the sample data provided in the Gatling section, the following template could be applied:\n\nResulting in a database full of entries like the following:\n\nTo learn more about how InfluxDB handles Graphite input, see Graphite Input.\n\nThe following diagram illustrates the interactions between each component referenced in the Design.\n\nThe directory structure of the performance tests is organized by category and test.\nThat is, each file is located in a directory named after the specific test which is itself located in a directory named after the category of test.\nFor example, the repose configuration files directory:\n\ncontains the category sub-directories filters/, services/, and use-cases/.\n\nArmed with this information, these are the basic steps to create a new test:\n\nThe dir_prefix is the {category}/{test} used to create the new vars file (e.g., filters/saml).\n\nThe package is similarly created using {category}.{test} (e.g., filters.saml).\n\nThe name should follow the Camel Cased pattern of {Test}{Category}Simulation (e.g., SamlFilterSimulation, ComplexUseCaseSimulation).\n\nThe Framework uses Ansible and it supports Jinja2 templates.\nThese templates allow users to add dynamic content to files and Ansible variables.\nJinja2 templated files must have a .j2 file extension appended to the file name and it will be removed as part of the templating process.\n\nA Jenkins job is triggered any time that code on the master branch of the Repose Git Repository changes.\nThe Jenkins job will run the performance tests and publish the results automatically.\nDesign describes the full automated process in more detail.\n\nIf you have the appropriate permissions on the Repose Jenkins instance, you can run the performance-test Jenkins job to manually start a single performance test.\n\nWe use Puppet to manage our performance test environment.\nThe details of our environment can be found in our Puppet manifest.\nIf you have Puppet installed, you can download our manifest and re-create our environment by running puppet apply directly on the downloaded file.\n\nGrafana serves as the user interface to performance test data which can be seen in the Performance Tests dashboard.\n\nAfter test execution has completed, Gatling will generate an HTML report.\nThe report can be found inside an archive residing in the home directory with a file name like SamlFilterSimulationRepose-1496319945650.tar.gz.\nThe exact file name will be included in the output of the Fetch the Gatling results task which will resemble the following snippet:\n\nAfter the report archive is located, the contents must be extracted before the report can be viewed.\nThat can be done with a command like the following:\ntar xvf /home/perftester/SamlFilterSimulationRepose-1496319945650.tar.gz\n\nOnce the report has been extracted, it can be viewed by opening the index.html file with a web browser.\n\nIf you need to manually run Gatling, you can do so from the Gatling host.\n\nFiles and directories of interest:\n\nThe Ansible directory is prepared by running the Gradle task buildAnsibleDirectory.\nThe base ansible files are processed using an ant based templating, this allows any build time values to be templated in (such as the gatling version to use).\nThe simulations are copied in from the source directory.\nAny dependencies of the simulations will be copied into place for placement in the Gatling lib directory.\n\nTo reduce redundancy, the Repose team created the performance-test-framework for Gatling.\nThis library includes the AbstractReposeSimulation, which is meant to serve as the basis for most, if not all, simulations.\n\nTo utilize the performance-test-framework, its JAR file must be placed in the Gatling lib directory.\nThis is handled along with any other dependencies by the Gradle task buildAnsibleDirectory.\nThe lib directory is a sub-directory of the Gatling home directory.\n\nIn the context of the Framework, Jenkins will build the performance-test-framework and copy the resulting JAR into the Ansible directory structure where it will then be copied to the hosts running Gatling.\n\nRun the simple use-case test for 15 minutes with a 5 minute warmup (20 minutes total):\n\nBy default, both the Repose test and the origin service test (i.e., Repose is bypassed) are run.\nYou can use Ansible tags to specify that only one of those tests should be run.\n\nYou can specify which cloud server flavor to use in the configuration overrides.\n\nBy default, the Repose start task will wait 5 minutes for Repose to start up.\nIf you expect Repose to take longer to start (e.g., due to a large WADL), you can increase this timeout using a command like the following:\n\nTo get Repose to use Saxon, add a saxon-license.lic file to the repose-aggregator/tests/performance-tests/roles/repose/files/ directory and pass in the following configuration override:\n\nYou can set the JVM Options (JAVA_OPTS) used by Repose by setting the following configuration override:\n\nUse the master playbook to run a performance test and immediately clean up afterwards without having to running the individual playbooks.\n\nYou can re-run a test using the same cloud resources by simply running the start_test.yml playbook again.\nYou can even specify different configuration overrides in subsequent runs, although there are some limitations.\nFor example, you can enable Saxon for a subsequent run, but you can\u2019t disable it afterwards.\nAlso, if you don\u2019t want the Repose JVM already warmed up from the previous run, you should have Ansible restart the Repose service.\nThis feature is considered experimental.\n\nYou can specify the base name of the directory containing the Gatling results in the configuration overrides.\nFor example, if you wanted the base name custom-base-name (resulting in a directory name resembling custom-base-name-repose-1487812914968), you would run:\n\nRunning a performance test with a unique naming prefix enables you to run a particular test multiple times simultaneously (each run requires a unique prefix):\n\nIf you\u2019re using the stop_test.yml playbook to clean up your cloud resources, you\u2019ll need to include the unique prefix to ensure the correct resources are deleted.\nIf the prefix is not specified, the wrong cloud resources or no cloud resources could end up being deleted, and in both cases, no error will be returned (due to idempotency).\n\nWhen running a Scripting filter test, the perf_test variable will look slightly different than it does for other tests.\nThe reason is that the scripting language to be used in the test is nested one level down in the perf_test variable value.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/add-header.html", "title": "9.0.1.0 | Filters | Add Header Filter", "content": "\nThe Add Header filter adds configured headers to a request and/or response.\nYou can use the filter to add new headers with specific values to a request chain, and it can also replace (i.e., remove any existing headers with the configured name) headers.\n\nThis filter does not require any request headers.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\n\nChanges to request headers vary based on configuration.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter does not modify the response body.\n\nChanges to response headers vary based on configuration.\n\nThis filter does not modify the response code.\n\nThis configuration will add a foo header to the request with a value of bar and a quality of 0.5.\nAny existing headers with the same name will not be removed.\n\nThis configuration will add a foo header to the response with a value of bar and a quality of 0.5.\nAny existing headers with the same name will be removed.\n\nThis configuration will add a foo header to the request with a value of bar and a quality of 0.8.\nAny existing request headers with the same name will be removed.\nIt will also add a baz header to the response with a value of qux and a quality of 0.4.\nAny existing response headers with the same name will not be removed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/api-validator.html", "title": "9.0.1.0 | Filters | API Validator Filter", "content": "\nThe API Validator filter validates all requests against a configured Web Application Description Language (WADL) file.\nFor example, if a request to the origin service is missing the tenant ID in the URI, but the origin service requires the tenant ID to be there, then the request will be rejected before it reaches the origin service.\nThe API Validator filter can also be configured to validate the content of the request body against an XML Schema Definition (XSD) or JSON Schema referenced as a grammar in the WADL.\n\nA working knowledge of WADL\u2019s and XSD\u2019s/JSON Schemas will make the configuration of this filter much easier.\nTherefore, it is recommended that you read up on these subjects before attempting to use this filter.\nA good tutorial on XSD 1.1, including a comparison of XSD 1.1 and XSD 1.0, is located at:\n\nMost of the ideas described in this conference session are implemented in this filter:\n\nThis filter does not require any request headers.\n\nHowever, the configuration can require specific headers and/or values be present in order to successfully exit this filter.\nThe most common use of a request header by this filter is for Role-Based Access Control (RBAC) using the X-Roles, X-Tenant-Id, and/or X-Map-Roles headers populated by either the Keystone v2 filter or the Keystone v2 Authorization filter.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\n\nHowever, due to the nature of this filter it is typically placed early in the filter chain immediately after any authentication filters.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThrough the use of the rax:code WADL extension, any of these default codes can be overridden to any value.\n\nThis configuration is the most basic example of this filter\u2019s configuration.\n\nEven though multiple validator element definitions and the role and default attributes on each are deprecated, the easiest way to future proof new configurations is to:\n\nThis configuration expands the basic example in order to show off all of the features of this element.\n\nTo place this filter in Delegation mode, add the delegating element to the filter configuration with an optional quality attribute that determines the delegating priority.\n\nThis configuration shows the deprecated, but currently legal, multi-validator definition as well as an embedded WADL which is also deprecated.\n\nThis filter is based on the API Checker library.\n\nThis component reports the following metrics to the Metrics Service:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/body-extractor-to-header.html", "title": "9.0.1.0 | Filters | Body Extractor to Header Filter", "content": "\nThis filter will extract a value from the request body and put it in a request header.\n\nThis filter does not require any request headers.\n\nThis filter has no dependency on any other filter.\n\nChanges to request headers vary based on configuration.\n\nThis filter does not modify the request body.\n\nThis filter is not a dependency of any other filter.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis filter does not modify the response status code.\n\nConsider the following example request body.\n\nThis configuration will extract user data from the example request body into headers.\n\nThis filter currently only supports JSON request bodies.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/body-patcher.html", "title": "9.0.1.0 | Filters | Body Patcher Filter", "content": "\nThis filter allows changes to the request/response body based on a path regex and content type.\n\nChanges take the form of JSON Patches as defined in RFC 6902.\n\nThis filter has no hard dependency on any other filter.\n\nHowever, the API Validator filter can be used to validate the request body before attempting to apply patches.\n\nThis filter does not create/modify any request headers.\n\nChanges to the request body vary based on configuration.\n\nThis filter is not a dependency of any other filter.\n\nChanges to the response body vary based on configuration.\n\nThis filter does not create/modify any response headers.\n\nThis configuration will apply a set of patches to HTTP messages for various resources.\nIn doing so, it will demonstrate all of the functions of this filter.\n\nThis filter currently only supports JSON bodies.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/compression.html", "title": "9.0.1.0 | Filters | Compression Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/content-type-stripper.html", "title": "9.0.1.0 | Filters | Content Type Stripper Filter", "content": "\nThis filter removes the Content-Type header when no content body is present.\n\nIf the first eight bytes of the content body are white space, then the body is considered absent.\nWhen a content body is present, requests will pass through to the next filter or the origin service without alteration.\n\nThis filter does not require any request headers.\n\nThis filter has no dependency on any other filter.\n\nThis filter does not modify the request body.\n\nThis filter is not a dependency of any other filter.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis filter does not modify the response code.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/cors.html", "title": "9.0.1.0 | Filters | CORS Filter", "content": "\nThe Cross-Origin Resource Sharing (CORS) filter allows Repose to manage CORS requests without the origin service needing to understand CORS.\nFor an introduction to CORS, see the CORS Overview section below.\n\nBefore enabling the CORS filter, you should be familiar with the CORS specification and the associated security implications.\nEnabling CORS can increase an endpoint\u2019s exposure.\nConfiguring CORS improperly could expose Cross-Site Request Forgery (CSRF) vulnerabilities in the origin service that were previously hidden by not supporting CORS.\n\nThe presence and values of certain headers will indicate what kind of request is being made.\nAll of these headers will be supplied by the client making the request.\n\n* If the Origin header matches the original Host header in the request, it will be considered a non-CORS request.\nSee the Origin Header section for more details.\n\nThe CORS filter should be one of the first filters, if not the first filter, in the filter chain in order to properly handle CORS Preflight Requests and to properly handle exposing response headers in CORS Actual Requests.\nIf you want to rate limit CORS Preflight Requests, you can add the following filters before the CORS filter in the filter chain:\n\nThis Rate Limiting filter would be in addition to any Rate Limiting filter you may already have in place.\nThe first Rate Limiting filter would filter only by IP address, and the second Rate Limiting filter would continue to rate limit the way it\u2019s currently set up.\n\nThis filter does not create/modify any request headers.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter does not modify the response body, but it will set it in certain early termination conditions.\nSee the [Response status codes] section for more details.\n\nThe filter will add the following headers to the response when the request is allowed to proceed:\n\nThe following headers will be added to the response when the request is rejected by this filter:\n\nThese are the conditions in which the CORS filter will stop processing the request and immediately return a response:\n\nA basic CORS configuration would allow all origins to use any standard HTTP method on all resources.\n\nIf you configure an origin regex of .* or any other regex that could allow untrusted origins, you may want to consider securing your API from CSRF exploits.\n\nTo limit which origins are allowed to initiate a CORS request to your API, you can specify a literal value or a regular expression that the Origin header must match in order to proceed with the request.\n\nIf specific resources support additional HTTP methods, you can configure this per-resource using a regex to specify the path or paths.\nThe resource configuration is processed in the configured order, so the first path regex to match the request URI will be used in conjunction with the global allowed-methods configuration.\nThis is used to determine the complete list of allowed methods to return in response to a CORS Preflight Request and to determine whether or not a CORS Actual Request is allowed to proceed past this filter.\n\nThis is not a substitution for authorization.\nRequests that do not contain the Origin header are not CORS requests and completely bypass this validation.\n\nUsing this configuration, you would see the following behavior:\n\nThis is an example configuration with notes on all of the required and optional elements and attributes.\n\nCORS is not a security feature.\nIt is a mechanism for informing clients (e.g., web browsers) of conditions when client-side security may be slightly relaxed in certain circumstances.\nThat is, the security lies completely within the client.\nSimply leaving out the Origin header in the request completely bypasses the CORS spec (and thus this filter).\nYou should continue securing your API in other ways using proper authentication and authorization mechanisms.\n\nFor security purposes, web browsers follow the Same-Origin policy.\nIf a user were to visit a website containing malicious code, the web browser would prevent the malicious code from trying to send requests to different websites on the user\u2019s behalf.\nThis is especially useful when the user is authenticated on those other websites.\nHowever, sometimes a website needs to be able to get data or perform an action on a different website, but how can the client know which websites allow this and under what circumstances?\nThis is where CORS comes in.\n\nInstead of the web browser immediately dropping any attempt to send a request to a third-party server, it can send the request to that server with CORS headers to see if the server trusts the origin server.\nThe address of the origin server is sent in the Origin header.\nIf the response from the third-party server does not contain the appropriate CORS headers (i.e., the server is not CORS aware) or if the CORS headers indicate the Origin is not allowed to send requests to it, the browser will drop the response (i.e., the client-side code from the origin server will not get to see the contents of the response from the third-party server).\n\nEven though the web browser will prevent the client-side code from seeing the response from the third-party server, the request may have still been processed by the third-party server.\nTo mitigate this issue, the web browser will send a CORS Preflight Request to the third-party server to first verify that the Origin and HTTP method are allowed (among a few other things) before sending the CORS Actual Request.\nIf the response to the CORS Preflight Request indicates the CORS Actual Request would not include the appropriate CORS headers, the web browser will not proceed with sending the CORS Actual Request.\n\nBecause the CORS Preflight Request is asking if a CORS Actual Request would be allowed and not for the request to actually be processed, this type of request is completely handled by the CORS filter in Repose.\nNo other filters after the CORS filter will process the request, and the request will not reach the origin service.\n\nA web browser may choose to skip sending a CORS Preflight Request if the HTTP method is GET, HEAD, or POST, the request headers do not include anything other than Accept, Accept-Language, and Content-Language, and the request does not require any cookies, HTTP authentication, nor use of any client-side SSL certificates.\nOtherwise, the web browser must make a CORS Preflight Request.\nFor example, if your origin service requires an X-Auth-Token header, the web browser will always send a CORS Preflight Request before sending the CORS Actual Request.\n\nSome web browsers (e.g., Chrome and Safari) will send the Origin header for same-origin (i.e., non-CORS) requests in addition to CORS requests which is technically allowed under RFC 6454.\nThis is typically not a concern for servers handling CORS because the unnecessary inclusion of CORS headers will be ignored by the client if they are not needed to process the response.\nBecause Repose has the added ability to reject requests with unapproved origins, additional logic is required to differentiate between CORS requests and same-origin requests when the Origin header is present.\n\nRequests are considered same-origin requests when the Origin header matches the original Host header set by the client according to the comparison rules in RFC 6454 Section 5.\nIf the X-Forwarded-Host header is present in the request, the first value will be used as the host.\nIf the header is not present or it cannot be parsed as a URI (when the URI scheme is prepended to it), then the Host header will be used instead.\nThis check is not performed for CORS Preflight Requests since web browsers should not be sending a CORS preflight header for a same-origin request.\n\nIf a proxy server sitting between the client and Repose rewrites the Host without updating the X-Forwarded-Host header with the original value, Repose will not be able to correctly identify same-origin requests coming from that client.\nThis may result in requests from that client being incorrectly rejected on the basis that they are CORS requests when they may in fact be same-origin requests.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/derp.html", "title": "9.0.1.0 | Filters | Delegation Response Processor (DeRP) Filter", "content": "\nThis filter is considered deprecated and will be removed in a future version.\nConsider using the HTTP Logging Service instead.\n\nThe Delegation Response Processor (DeRP) filter rejects any request containing the X-Delegated header.\nThe content of the response will match is determined by the content of the X-Delegated header.\n\nThis filter is part of the delegation recipe.\n\nThis filter does not require any request headers.\n\nHowever, the following headers are used during processing:\n\nThis  filter does not require any preceding filters.\n\nHowever, it is expected that filters which support delegation (i.e., write a delegation header to the request) will precede this filter.\nIf no filter which supports delegation precedes this filter, this filter will forward all requests to the next component without modification.\n\nAdditionally, the HERP filter may precede this filter as part of the delegation recipe.\n\nThis filter does not create/modify any request headers.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nWhen delegation is used, this filter will terminate the processing chain preventing succeeding filters from running.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis filter will only modify the response status code when rejecting a delegated request (i.e., if the request contains a delegation header).\nThe response status code will be set to status code associated with the highest quality delegation value, or 500 if the delegation request header exists but is not parsable.\nThe response reason phrase will be set to the message associated with the highest quality delegation value.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/destination-router.html", "title": "9.0.1.0 | Filters | Destination Router Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nThis component reports the following metrics to the Metrics Service:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/echo.html", "title": "9.0.1.0 | Filters | Echo Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/exception.html", "title": "9.0.1.0 | Filters | Exception Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/forwarded-proto.html", "title": "9.0.1.0 | Filters | Forwarded Protocol Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/header-normalization.html", "title": "9.0.1.0 | Filters | Header Normalization filter", "content": "\nThe Header Normalization filter removes configured headers from a request and/or response.\nNormalization is the process of modifying or standardizing content to optimize the flow of information.\nThe Header Normalization filter normalizes the headers of the request by performing two separate functions.\nThe filter uses blacklisting to prevent specific headers and it uses whitelisting to allow only approved headers from continuing down the filter chain.\nThe headers can be matched by URI regular expression (uri-regex) and/or HTTP method type (http-methods).\n\nThis filter does not require any request headers.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\nHowever, due to the nature of this filter it is typically placed early in the filter chain.\n\nChanges to request headers vary based on configuration.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter does not modify the response body.\n\nChanges to response headers vary based on configuration.\n\nThis filter does not modify the response code.\n\nThis configuration shows how to use the Whitelist feature of the Header Normalization filter.\nIt will remove all other headers except those required to initiate an authentication sequence.\n\nThis configuration shows how to use the Blacklist feature of the Header Normalization filter.\nIt will remove only the specified headers from all responses.\n\nThis configuration is a more complex example.\n\nEvery target that matches will get evaluated.\n\nThis component reports the following metrics to the Metrics Service:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/header-translation.html", "title": "9.0.1.0 | Filters | Header Translation Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/header-user.html", "title": "9.0.1.0 | Filters | Header User Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/herp.html", "title": "9.0.1.0 | Filters | Highly Efficient Record Processor (HERP) Filter", "content": "\nThis filter is considered deprecated and will be removed in a future version.\nConsider using the HTTP Logging Service instead.\n\nThe Highly Efficient Record Processor (HERP) filter logs an event for each API request.\nThese logs provide information regarding the processing of each request.\nThe logs can be inspected to view the usage of a service and to perform auditing.\nEvents are recorded even if the API request is rejected by Repose before reaching the origin service.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\n\nIn order for the HERP filter to process and record every request, no preceding filter in the filter chain may write a response.\nTo prevent short-circuiting without changing behavior, delegation has been added to many Repose filters.\nSee Delegation in Repose for more details.\n\nThis filter does not create/modify any request headers.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\nHowever, the DeRP filter commonly follows the HERP filter.\n\nThe HERP filter will not process responses sent from preceding filters.\nThe DeRP filter can be used to perform delegation.\nPlace the DeRP filter after the HERP filter so that responses sent by any preceding filters are delegated.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis filter does not modify the response code.\n\nTo use the HERP filter, you need to:\n\nThe HERP filter allows user-configurable filtering along with optional pre and post log identifiers.\n\nEach filterOut block contains all of the criteria required for a single filter.\nIf any of the individual criterion in the match elements contained within a filterOut element fail, then that filter fails and it goes on to evaluate the next one.\nIf any of the filterOut blocks fully match, then the event is filtered and not logged to the post-filter log.\nIn other words, the match elements are logically AND\u2019d and the filterOut blocks are logically OR\u2019d.\n\nThe {{methodLabel}} variable value is provided by the api-validator filter.\nIf you aren\u2019t using that filter, {{methodLabel}} will return empty string.\n\nThe HERP filter allows user-configurable filtering along with optional pre and post log identifiers.\n\nEach filterOut block contains all of the criteria required for a single filter.\nIf any of the individual criterion in the match elements contained within a filterOut element fail, then that filter fails and it goes on to evaluate the next one.\nIf any of the filterOut blocks fully match, then the event is filtered and not logged to the post-filter log.\nIn other words, the match elements are logically AND\u2019d and the filterOut blocks are logically OR\u2019d.\n\nFor information about using the Flume with the HERP filter for user access event recording, see CF Flume Sink for more details.\n\nIn the template element, certain template keys may be used to add dynamic content to the text being logged.\nAny valid key enclosed in brackets (i.e., {{<key>}}) will be replaced, brackets included, by the value associated with that key at runtime.\nNote that key names are case sensitive.\nA list of supported keys follows:\n\nThe HERP filter also provides helper functions to the templating engine.\nThese helpers work like the keys above, but take some input.\nFor example, assuming the HTTP request method is a GET, {{cadfMethod requestMethod}} would be replace by \"read/get\".\nThe following helpers are available:\n\nIf you expect heavy usage, you should use a logging tool, such as Logstash, for managing events and logs and should not write events to files.\nEven if you rely on auditing, it is not recommended that you use the file system with the audit logs.\nRepose has developed CF Flume Sink to eliminate disk space and log file issues.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/ip-user.html", "title": "9.0.1.0 | Filters | IP User Filter", "content": "\nThe IP User filter populates request headers using the client\u2019s IP address and matching configured group, if any.\n\nThis filter does not require any request headers.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\nHowever, due to the nature of this filter it is typically placed early in the filter chain.\n\nThe name of these headers can be changed from these default values in the configuration.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\nHowever, the following filters may be useful:\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis configuration will provide the default headers using the client\u2019s IP Address.\n\nThere must be at least one group element defined.\n\nThis configuration shows explicitly naming the user and group headers to their default values and setting their qualities to the default also.\n\nIF present, THEN the user-header must be the first element.\n\nIF present, THEN the group-header must be the element directly before the first group element.\n\nThere must be at least one group element definition.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/iri-validator.html", "title": "9.0.1.0 | Filters | IRI Validator Filter", "content": "\nThis filter verifies that the request URI has been properly converted from an IRI.\n\nIn other words, in the case where a request URI is a valid URI but, after conversion, is an invalid IRI, this filter will reject the request.\n\nThis filter does not require any request headers.\n\nThis filter has no dependency on any other filter.\n\nThis filter does not create/modify any request headers.\n\nThis filter does not modify the request body.\n\nThis filter is not a dependency of any other filter.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/keystone-v2-authorization.html", "title": "9.0.1.0 | Filters | Keystone v2 Authorization Filter", "content": "\nThe Keystone v2 Authorization Filter authorizes requests based on data about the user making the request.\n\nThe following headers are expected to be populated by the Keystone v2 Filter.\n\nThis filter expects that the Keystone v2 Filter precede it to populate necessary user data.\nBe aware, however, that the necessary user data is transported via request header, and thus, could could be populated elsewhere.\n\nIt is best practice to prevent spoofing by putting the Header Normalization filter before any authentication and/or authorization filters so that it can remove any headers that would be populated by them.\n\nThe following headers are created:\n\nIf delegation is enabled, then the X-Delegated header is created.\nThis is mainly intended for use by the Highly Efficient Record Processor (HERP) filter and Delegation Response Processor (DeRP) filter for internal delegation processing within Repose.\nHowever, it can be exposed to the origin service under certain configurations.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\nHowever, the following filters may be useful:\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis configuration will validate the tenant ID(s) in the request against the tenant ID(s) in the user data.\nAll tenants extracted from the request must have at least one matching tenant in the user data.\n\nThis configuration will validate that the user data contains a configured endpoint.\n\nYou can configure this filter to allow no-op processing of requests that do not require authorization.\nFor example, a service might want all calls authorized with the exception of the call for WADL retrieval.\nIn this situation, you can configure the whitelist as shown in the example below.\nThe whitelist contains a list of Java Regular Expressions that Repose attempts to match against the full request URI.\nIf the URI matches an expression in the white list, then the request is passed to the origin service.\nOtherwise, authorization is performed against the request.\n\nPre-authorized roles can be configured to allow a user to bypass authorization if the user data contains a role matching one of the pre-authorized roles.\n\nIn some cases, you may want to delegate the decision to reject a request down the chain to either another filter or to the origin service.\nThis filter allows a request to pass as either Confirmed or Indeterminate when configured to run in delegating mode.\nTo place the filter in delegating mode, add the delegating element to the filter configuration with an optional quality attribute that determines the delegation priority.\nWhen in delegating mode, the filter sets the X-Identity-Status header with a value of Confirmed when a user has been authorized by the this filter and to Indeterminate when a user was not authorized by this filter.\nThe the X-Identity-Status header is in addition to the regular X-Delegated delegation header being created.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/keystone-v2-basic-auth.html", "title": "9.0.1.0 | Filters | Keystone v2 Basic Auth Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/keystone-v2.html", "title": "9.0.1.0 | Filters | Keystone v2 Filter", "content": "\nProvides a mechanism for authenticating and enriching requests with data from an OpenStack Keystone v2 Identity service.\n\nThis filter has no dependencies on other filters.\n\nHowever, it is a good practice to prevent spoofing of identities by putting the Header Normalization filter before any authentication and/or authorization filters so that it can remove any headers that would be populated by them.\n\nThe following headers are created using the information returned from the authentication service:\n\nThe following headers are added for use by the Rate Limiting filter:\n\nIf the set-catalog-in-header attribute is true, then the service catalog from the authentication service is a base 64 encoded and placed in the following header:\n\nThe Keystone v2 Identity service supports impersonation.\nWhen an impersonation token is validated, the authentication service will return identifying information for the impersonator.\nThis information allows impersonated calls to be tracked (e.g., via SLF4J HTTP Logging filter).\nThe origin service can also determine when a request is impersonated and who the impersonator is.\nThe information is placed in the following headers:\n\nThe Keystone v2 Identity service also has other attributes it provides when a token is validated.\nIf any of this information is provided, then it will be passed in the following headers:\n\nIf delegation is enabled, then the X-Delegated header is created.\nThis is mainly intended for use by the Highly Efficient Record Processor (HERP) filter and Delegation Response Processor (DeRP) filter for internal delegation processing within Repose.\nHowever, it can be exposed to the origin service under certain configurations.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\nHowever, the following filters may be useful:\n\nThis filter does not modify the response body.\n\nSuccessful (2xx)\n\nRequest continues\n\nRequest continues\n\nRequest continues\n\n400\n\n500\n\n500\n\n500\n\n401\n\n500\n\n500\n\n500\n\n401\n\n401\n\n401\n\n403\n\nThe admin token is unauthorized.\n\n500\n\n500\n\n500\n\n404\n\n401\n\n401\n\nRequest continues\n\n405\n\n500\n\n500\n\n500\n\n413\n\n429\n\nThe Keystone v2 Identity service rate limited the Repose instance.\n\n503\n\n503\n\n503\n\n500\n\n501\n\n502\n\n503\n\nThe Keystone v2 Identity service failed to process the request.\n\n502\n\n502\n\n502\n\nThis configuration will provide the basic headers using self-validating tokens.\n\nThis configuration will use an admin account instead of using the self-validating tokens feature.\n\nIF either a username OR a password is supplied, THEN you must provide both a username AND a password.\n\nThis configuration is an example using the identity-service element\u2019s configuration attributes that have not yet been shown in an example.\n\nIn some cases, you may want to delegate the decision to reject a request down the chain to either another filter or to the origin service.\nThis filter allows a request to pass as either confirmed or indeterminate when configured to run in delegating mode.\nTo place the filter in delegating mode, add the delegating element to the filter configuration with an optional quality attribute that determines the delegating priority.\nWhen in delegating mode, the filter sets the X-Identity-Status header with a value of confirmed when valid credentials have been authenticated by the Keystone v2 Identity service and to indeterminate when the credentials are not.\nThe the X-Identity-Status header is in addition to the regular X-Delegated delegation header being created.\n\nYou can configure this filter to allow no-op processing of requests that do not require authentication.\nFor example, a service might want all calls authenticated with the exception of the call for WADL retrieval.\nIn this situation, you can configure the whitelist as shown in the example below.\nThe whitelist contains a list of Java Regular Expressions that Repose attempts to match against the full request URI.\nIf the URI matches an expression in the white list, then the request is passed to the origin service.\nOtherwise, authentication is performed against the request.\n\nThis filter caches authentication tokens.\nThe length of time that tokens are cached is determined by the Time To Live (TTL) value that is returned from the authentication service (e.g., the Keystone v2 Identity service) during token validation.\n\nYou can configure alternate maximum TTL for caching of authentication tokens, groups, and endpoints.\nIf you specify the token element value in the configuration file, this value is used when caching tokens, unless the token TTL value provided by the Keystone v2 Identity service is less than the token-cache-timeout value.\nThis method prevents Repose from caching stale tokens.\nIf the token\u2019s TTL exceeds the maximum allowed TTL value (2^31 - 1), the maximum allowed TTL is used.\n\nEach timeout value behaves in the following way:\n\nYou can configure this filter to use an Atom Feed for cache expiration.\nThis configuration blocks malicious users from accessing the origin service by repeatedly checking the Cloud Feed from the authentication service.\nTo set up this filter to use Cloud Feeds for cache expiration, you will need to enable the Atom Feed Consumption service in the System model, configure the Atom Feed Consumption service, and configure this filter with which feeds to listen to.\n\nThe Rackspace infrastructure uses Cloud Feeds (formerly Atom Hopper) to notify services of events.\nThis is not default OpenStack behavior, and may require additional services for use.\nA list of Rackspace Cloud Feeds endpoints for Identity Events can be found at\nthe internal Rackspace Wiki page linked here.\n\nTenant validation has been moved to the Keystone v2 Authorization Filter, and is considered deprecated in this filter.\n\nTenant ID Validation is the capability of this filter to parse a tenant ID out of the request and validate it against the tenant ID(s) available in the response token from the Keystone v2 Identity service.\n\nThe header denoted by the header-extraction-name element can be populated using filters like the URL Extractor to Header Filter and Body Extractor to Header Filter.\n\nIf the default tenant and a required tenant are the same, then the highest quality between the two will be used.\n\nIf the validate-tenant element is not present, then this filter will not attempt to validate a Tenant ID from the request.\n\nThere can be multiple header-extraction-name elements.\nThis enables validation of tenant IDs from varying sources.\nAll tenant IDs extracted from request headers will be validated.\n\nPre-authorized roles have been moved to the Keystone v2 Authorization Filter, and are considered deprecated in this filter.\n\nIf Tenant ID Validation is enabled, then a list of roles that are allowed to bypass this check can be configured.\nThese configured roles will be compared to the roles returned in a token from the Keystone v2 Identity service, and if there is a match, the Tenant ID check will be skipped.\n\nService endpoint requirements have been moved to the Keystone v2 Authorization Filter and are considered deprecated in this filter.\n\nIf endpoint authorization is enabled, then the user must have an endpoint in their catalog meeting the defined criteria.\n\nThe region, name, and type attributes are all optional and can be combined as needed to achieve the desired restrictions.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/merge-header.html", "title": "9.0.1.0 | Filters | Merge Header Filter", "content": "\nThe Merge Header Filter merges multiple header lines with the same name into a single header line with multiple comma-separated values.\n\nThis filter can process both requests and responses.\n\nThis filter does not require any request headers.\n\nThis filter has no dependency on any other filter.\n\nChanges to request headers vary based on configuration.\n\nExisting request headers may be merged into a single line.\n\nThis filter does not modify the request body.\n\nThis filter has no recommended follow-on filters.\n\nThis filter does not modify the response body.\n\nChanges to response headers vary based on configuration.\n\nExisting response headers may be merged into a single line.\n\nThis filter does not modify the response code.\n\nThis configuration will merge the Accept and X-Roles headers on the request, and the Cache-Control header on the response.\n\nFor example, if a request is made with the header lines:\n\nThis filter will merge those header lines into the header line:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/openapi-validator.html", "title": "9.0.1.0 | Filters | OpenAPI Validatort Filter", "content": "\nThe OpenAPI Validator Filter validates requests against a configured OpenAPI Document.\n\nThe OpenAPI document describes an API, detailing how to make requests and what form the responses will take.\nIf a request fails validation against the OpenAPI document, this filter will reject the request.\nFor example, if the OpenAPI document requires a query parameter for a particular operation and the request for that operation is missing the query parameter, this filter will discontinue processing of the request and send an error response.\n\nVary based on configuration.\n\nThis filter itself does not require any request headers, however the configured OpenAPI document may.\n\nThis filter has no dependency on any other filter.\n\nThis filter does not create/modify any request headers.\n\nThis filter does not modify the request body.\n\nThis filter is not a dependency of any other filter.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis configuration will validate inbound requests against an OpenAPI document.\n\nThis filter is backed by the Swagger Request Validator library.\nAs such, the validations supported by this filter are exactly those supported by that library.\n\nIf a request fails validation due to more than one issue, the issue with the highest priority will be used to set the response status code and reason.\n\nThe following list provides all request validation issues handled by this filter in priority order (i.e., issues higher on the list will be used over issues lower on the list):\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/openstack-identity-v3.html", "title": "9.0.1.0 | Filters | Openstack Identity v3 Filter", "content": "\nThe OpenStack Identity v3 Filter authenticates, authorizes, and enriches requests using data from an OpenStack Identity v3 service.\n\nAuthentication is the process of validating that a request was made by a valid user.\nThis is accomplished by checking that the subject token on the request can be confirmed by the identity service.\n\nAuthorization is the process of validating that the user has permission to make a request.\nThis is accomplished by checking two factors:\n\nAuthorization is an optional feature.\n\nEnrichment is the process of adding additional information to the request.\nThis information is gathered through authentication and authorization.\nIt includes things like the user\u2019s name, ID, roles, groups, and service catalog.\n\nThis filter has no required preceding filters, however the following filters may be useful:\n\nThe following headers are created using the authentication information returned from the identity service:\n\nThe following headers are added for use by the Rate Limiting filter:\n\nIf the forward-catalog attribute is true, then the service catalog from the identity service is base 64 encoded and placed in the following header:\n\nSome instance of the OpenStack Identity v3 service may support impersonation.\nWhen an impersonation token is validated, the identity service will return identifying information for the impersonator.\nThis information allows impersonated calls to be tracked (e.g., via SLF4J HTTP Logging filter).\nThe origin service can also determine when a request is impersonated and who the impersonator is.\nThe information is placed in the following headers:\n\nDelegation is a way to continue processing a request despite the occurrence of an error.\nIt is mainly intended for use by the Highly Efficient Record Processor (HERP) filter and Delegation Response Processor (DeRP) filter for internal delegation processing within Repose.\nHowever, it can be exposed to the origin service under certain configurations.\nIf delegation is enabled via the inclusion of the delegating element in the configuration, the following headers may be added to the request:\n\nThis filter does not modify the request body.\n\nThis filter has no required succeeding filters, however the following filters may be useful:\n\nThis filter does not modify the response body.\n\nSuccessful (2xx)\n\nRequest continues\n\nRequest continues\n\nRequest continues\n\n400\n\n500\n\n500\n\n500\n\n401\n\nThe admin credentials are invalid.\n\n500\n\n500\n\n500\n\n403\n\nThe admin token is unauthorized.\n\n500\n\n500\n\n500\n\n404\n\n500\n\n401\n\n500\n\n405\n\n500\n\n500\n\n500\n\n413\n\n429\n\nThe OpenStack Identity v3 service rate limited the request from Repose.\n\n503\n\n503\n\n503\n\n500\n\n501\n\n502\n\n503\n\nThe OpenStack Identity v3 service failed to process the request.\n\n500\n\n500\n\n500\n\nThis configuration will authenticate a user and provide basic user information in headers.\n\nThis configuration is an example using the openstack-identity-v3 and openstack-identity-service elements' optional configuration attributes.\n\nIn some cases, you may want to delegate the decision to reject a request down the chain to either another filter or to the origin service.\nThis filter allows a request to pass as either confirmed or indeterminate when configured to run in delegating mode.\nTo place the filter in delegating mode, add the delegating element to the filter configuration with an optional quality attribute that determines the delegating priority.\nWhen in delegating mode, the filter sets the X-Identity-Status header with a value of confirmed when valid credentials have been authenticated by the OpenStack Identity  v3 service and to indeterminate when the credentials are not.\nThe the X-Identity-Status header is in addition to the regular X-Delegated delegation header being created.\n\nYou can configure this filter to allow no-op processing of requests that do not require authentication.\nFor example, a service might want all calls authenticated with the exception of the call for WADL retrieval.\nIn this situation, you can configure the whitelist as shown in the example below.\nThe whitelist contains a list of Java Regular Expressions that Repose attempts to match against the full request URI.\nIf the URI matches an expression in the white list, then the request is passed to the origin service.\nOtherwise, authentication is performed against the request.\n\nThis filter caches authentication tokens and user groups.\nThe length of time that tokens are cached is determined by the Time To Live (TTL) value that is returned from the OpenStack Identity v3 service during token validation.\n\nYou can configure alternate maximum TTL for caching of authentication tokens and groups.\nIf you specify the token element value in the configuration file, this value is used when caching tokens, unless the token TTL value provided by the OpenStack Identity v3 is less than the configured value.\nThis method prevents Repose from caching stale tokens.\nIf the token\u2019s TTL exceeds the maximum allowed TTL value (2^31 - 1), the maximum allowed TTL is used.\n\nEach timeout value behaves in the following way:\n\nYou can configure this filter to use an Atom Feed for cache expiration.\nThis configuration blocks malicious users from accessing the origin service by repeatedly checking the Cloud Feed from the authentication service.\nTo set up this filter to use Cloud Feeds for cache expiration, you will need to enable the Atom Feed Consumption service in the System model, configure the Atom Feed Consumption service, and configure this filter with which feeds to listen to.\n\nThe Rackspace infrastructure uses Cloud Feeds (formerly Atom Hopper) to notify services of events.\nThis is not default OpenStack behavior, and may require additional services for use.\nA list of Rackspace Cloud Feeds endpoints for Identity Events can be found at\nthe internal Rackspace Wiki page linked here.\n\nProject ID authorization is the capability of this filter to parse a tenant ID out of the request and validate it against the project ID(s) available in the response token from the OpenStack Identity v3 service.\n\nIf the default project ID and a project ID extracted from the request are the same, then the highest quality between the two will be used.\n\nIf the validate-project-id-in-uri element is not present, then this filter will not attempt to validate a project ID from the request.\n\nIf project ID authorization is enabled, then a list of roles that are allowed to bypass this check can be configured.\nThese configured roles will be compared to the roles returned in a token from the OpenStack Identity v3 service, and if there is a match, the project ID check will be skipped.\n\nIf endpoint authorization is enabled, then the user must have an endpoint in their catalog meeting the defined criteria.\n\nThe region, name, and interface attributes are all optional and can be combined as needed to achieve the desired restrictions.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/rate-limiting.html", "title": "9.0.1.0 | Filters | Rate Limiting Filter", "content": "\nThe Rate Limiting filter can be used to limit the number of requests that make it through Repose to specified resources belonging to the origin service.\nBy imposing a rate limit, an operator can prevent the origin service from being flooded by requests, thereby bounding the potential for congestion or contention.\n\nAdditionally, this filter can expose rate limiting data by allowing users to query rate limits.\nWith accessible rate limiting data, a user can determine the number of requests they are allowed to make per period of time to various resources.\n\nSee Additional Information for more details on how rate limiting is performed.\n\nThis filter uses the quality parameter of header values to determine the user and group(s) provided in the request.\nThe value(s) with the highest quality will be used, while all other values will be ignored.\nIn the case of the X-PP-User header, only the first of the highest quality values will be used.\n\nThis is useful when an operator desires to employ multiple authentication/identity mechanisms.\n\nStrictly speaking, there is not a hard requirement for any filter to precede this filter.\nHowever, it is strongly recommended to utilize the following filters ahead of this filter.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter will only modify the response body if this filter is configured with a request-endpoint.\nIn that case, in response to requests made to the request-endpoint, this filter will populate the response body with details of the current state of limits which apply to the provided user with the provided group(s).\n\nSee Querying Limits for more details.\n\nIn this example, we have configured this filter to allow users to query their limits.\nProvided below are sample requests to give you a better idea of what that looks like.\n\nThis configuration provides per-user limits, global limits, and an endpoint to query limit data.\n\nA limit group is just a group containing one or more limit(s).\nThe motivation behind limit groups is to provide a way of applying different limits to different types of users.\nThe limits that will be applied to the request are the limits in the limit group configured with a group that matches a value in the X-PP-Groups header.\nIf no limit group matches the request, the default limit group will be used.\n\nWhen a limit in a group is reached, succeeding limits within the same limit group will never be applied.\nIn other words, the most restrictive limit will prevent updates to succeeding limits.\nPreceding limits within the group will continue to apply (i.e., continue to increment).\n\nOnly the limits in the first limit group to match the request are applied.\n\nA limit is the number of requests per unit of time that are allowed to pass through Repose.\nLimits are applied selectively to requests matching certain criteria such as a request\u2019s HTTP method and query parameters.\nMultiple rate limits that match the same request will all apply.\nA limit is reached when a user has sent a number of requests equal to the value of the limit within the span of one unit (configurable on the limit) of time.\nIf a limit has been reached, this filter will prevent requests from passing through Repose.\nIf a limit has not been reached, the request will pass through this filter.\nLimits will reset after the configured unit of time has passed.\n\nGlobal limits are very similar to normal Limits, except that they are applied to all requests, irrespective of the user making the request.\nThat is, global limits can protect the end service as a whole rather than just from individual users.\nFor example, if a service is known to only handle up to 500 requests per second in total, a global rate limit can be set to 500 requests per second to prevent further requests from reaching and compromising the origin service.\nAny request which breaks that limit will receive a response with status code 503.\n\nGlobal limits are not currently queryable via the request-endpoint.\n\nThis is the algorithm used to determine whether or not a limit has been reached.\nIn this algorithm, a time unit is considered discrete and independent from other units.\nTherefore, only requests that occur during such a time unit are counted towards the limit.\nOnce the end of a time unit is reached, the count toward a limit will be reset.\nThis is in contrast to a leaky bucket rate limit, wherein time units are continuous.\n\nIt is important to note that, with this approach, it is possible that 2x - 1 (where x is the configured limit value) requests will pass through this filter over the period of one time unit.\nIf that occurs, however, fewer requests will be allowed to pass through surrounding time units.\nWhich is to say that, on average, the configured rate limit of x will be enforced.\n\nConsider the following example.\nFive requests are allowed per minute for each time-block, but the one-minute window that lands in between the two units has seven requests.\nThis shows that limits may be broken for a given window of time, however, across multiple units of time, the allowed requests average less than or equal to five requests per minute.\n\nThis filter tracks limits by user, and consequently, requires a user be provided when querying limits.\nThe exception is global limits, however, since global limits are not currently query-able, they will be omitted from this explanation.\nTo query a limit, a GET request should be made to the request-endpoint.\nIf no request-endpoint is defined, then querying limits is not possible.\nAs always, requests must include the X-PP-User header, and may include the X-PP-Groups header.\nThe limit details returned from the request-endpoint will be for the limits in the limit group that matches a provided group as they apply to the provided user.\n\nThe limit details themselves will be presented in either XML or JSON format.\nIf no Accept header is present on the request, or if the Accept header has no value, the JSON format will be returned as the default.\nOtherwise, the returned format will be determined based on the Accept header specification defined in Section 14 of RFC 2616.\nIf the limit query request does not accept at least one of the supported media types, a 406 response will be returned.\n\nTake a look at the Querying Limits Example to see this feature in action!\n\nThe set of absolute limits is comprised of the set of limits defined in this filter\u2019s configuration and the limits imposed by the origin service.\nThese limits will only be returned if include-absolute-limits on the request-endpoint is set to true.\nOtherwise, only the limits defined in this filter\u2019s configuration will be returned.\n\nTo retrieve limits from a origin service, a request will be made through the Repose filter chain to the origin service.\n\nIf this filter is configured to use the Distributed Datastore, and the Distributed Datastore service is unable to communicate with other nodes (e.g., during a rolling upgrade to Repose), then the Distributed Datastore service will fall back on the local datastore.\nConsequently, rate limits may not be tracked correctly during the transition from the Distributed Datastore to the local datastore or vice-versa.\nTo be more precise, when Distributed Datastore nodes are unreachable, limit data cannot be be retrieved, and so new limits are created in the local datastore.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/regex-rbac.html", "title": "9.0.1.0 | Filters | RegEx Role Based Access Control (RBAC) Filter", "content": "\nThe RegEx Role Based Access Control (RBAC) filter provides a way to get API validation for services without the constraints of a WADL.\n\nThe Role-Based Access Control (RBAC) is achieved using a request header populated by an authentication filter.\n\nThis header is not configurable.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\n\nHowever, due to the nature of this filter, it is typically placed early in the filter chain immediately after any authentication filters (e.g., Keystone v2 Filter).\n\nThis header is only added if delegation is enabled.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis configuration is a basic example that exhibits a common use-case.\n\nThe resources are defined using Java Regular Expressions.\nThe requested resource endpoint must exactly match a RegEx to be considered a match.\nThat is to say, there can not be any extra characters in the request, including or excluding trailing slashes, that are not accounted for in the RegEx.\n\nANY and ALL act as special wildcards for both HTTP methods and roles.\n\nRoles are allowed to have spaces in the names through the use of an embedded Non-Breaking Space (NBSP) (i.e., Unicode character: 00A0).\nSimply input a non-breaking space, in the place where the role name should have a space.\nSee the following for more information: https://en.wikipedia.org/wiki/Non-breaking_space#Keyboard_entry_methods\n\nThis configuration is a basic example that exhibits a common use-case.\n\nThis configuration is a full example that uses every possible configuration item.\nIt does not, however, cover the resources format, as including resources both inline and via the href attribute will cause only the inline resources to be used.\n\nIn some cases (e.g., User Access Events), you may want to delegate the validation of a request down the chain to either another filter or to the origin service.\nDelegation prevents the RegEx RBAC filter from failing the request by forwarding the request with the X-Delegated header that is set with a value which indicates how the filter would have failed if not in delegating mode.\n\nTo place the filter in delegating mode, add the delegating element to the filter configuration with a quality that determines the delegation priority.\n\nThe format for the X-Delegated header value is:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/scripting.html", "title": "9.0.1.0 | Filters | Scripting Filter", "content": "\nThe Scripting filter enables users to write custom filters for Repose using a variety of scripting languages.\nCustom filters can be used to perform arbitrary processing with access to the bindings described below.\n\nThis filter does not require any request headers.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\n\nChanges to request headers vary based on configuration.\n\nChanges to the request body varies based on configuration.\n\nThis filter is not strictly required by any other filters.\n\nChanges to the response body varies based on configuration.\n\nChanges to response headers vary based on configuration.\n\nChanges to the response code varies based on configuration.\n\nAll supported languages are currently compilable.\n\nAn enumeration of all supported language names (e.g., python and jython) that will be accepted in configuration can be found in the XML schema linked on this page.\n\nBindings are variables defined by the Scripting filter which can be used in scripts.\n\nCurrently supported bindings are detailed by the following table:\n\nInvoking the doFilter method on the filterChain object is necessary to pass the request/response along.\nIf doFilter is not invoked, the scripting filter will return the response object as-is up the filter chain.\nNo later filter nor the origin service will have a chance to process the request.\n\nThe performance of this filter will vary depending on configuration.\n\nFor the simple task of adding a header to a request, this filter performs comparably to the Add Header Filter.\n\nScripts written in a compilable language as shown above will always be compiled.\nThis should dramatically improve performance.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/simple-rbac.html", "title": "9.0.1.0 | Filters | Simple Role Based Access Control (RBAC) Filter", "content": "\nThe Simple Role Based Access Control (RBAC) filter provides a way to get API validation for services without the need to create a WADL.\n\nThe Role-Based Access Control (RBAC) is achieved using a request header populated by an authentication filter.\n\nIn the original super simple legacy mode:\n\nOR\n\nIn the newer multi-tenant mode:\n\nThe most common use of a request header by this filter is for Role-Based Access Control (RBAC) using the X-Roles, X-Tenant-Id, and/or X-Map-Roles headers populated by either the Keystone v2 filter or the Keystone v2 Authorization filter.\n\nThis filter has no dependencies on other filters and can be placed wherever it is needed in the filter chain.\n\nHowever, due to the nature of this filter, it is typically placed early in the filter chain immediately after any authentication filters (e.g., Keystone v2 Filter).\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis configuration is a basic example that exhibits a common use-case.\n\nWhile roles are allowed to have spaces in the names, any additional leading and trailing whitespace around the comma separators is removed.\nThat is to say a role can NOT start or end with a space.\n\nANY and ALL act as special wildcards for both HTTP methods and roles.\nWhen used for HTTP methods it is shorthand for the four basic REST methods (GET, PUT, POST, and DELETE).\n\nThis configuration is a full example that uses every possible configuration item.\nIt does not, however, cover the resources format, as including resources both inline and via the href attribute will cause only the inline resources to be used.\n\nIn some cases, you may want to delegate the validation of a request down the chain to either another filter or to the origin service.\nDelegation prevents the Simple RBAC filter from failing the request by forwarding the request with the X-Delegated header that is set with a value which indicates how the filter would have failed if not in delegating mode.\n\nTo place the filter in delegating mode, add the delegating element to the filter configuration with a quality that determines the delegation priority.\n\nThe format for the X-Delegated header value is status_code={status-code}`component={filter-name}`message={failure message};q={delegating-quality}.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/slf4j-http-logging.html", "title": "9.0.1.0 | Filters | SFL4J HTTP Logging Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/split-header.html", "title": "9.0.1.0 | Filters | Split Header Filter", "content": "\nThe Split Header Filter splits header lines with multiple values into multiple header lines each with a single value.\n\nHeader values are split using the comma character (,) as a delimiter.\n\nThis filter can process both requests and responses.\n\nThis filter does not distinguish between headers that should and should not be split.\nAny header configured to be split will be split.\nCare should to taken to ensure that headers that should not be split (e.g., User-Agent) are not configured to be split.\nPlease refer to RFC7231 for details on standard HTTP headers and their syntax.\n\nThis filter does not require any request headers.\n\nThis filter has no dependency on any other filter.\n\nChanges to request headers vary based on configuration.\n\nExisting request headers may be split across multiple lines.\n\nThis filter does not modify the request body.\n\nThis filter has no recommended follow-on filters.\n\nThis filter does not modify the response body.\n\nChanges to response headers vary based on configuration.\n\nExisting response headers may be split across multiple lines.\n\nThis filter does not modify the response code.\n\nThis configuration will split the Accept and X-Roles headers on the request, and the Cache-Control header on the response.\n\nFor example, if a request is made with the header lines:\n\nThis filter will split those header lines into the header lines:\n\nThis filter may be used to imitate the the behavior of Repose when header splitting was performed as a core function rather than by this filter (prior to Repose v9).\nTo configure this filter for that use case, see the Repose < 9.0 Header Splitting Behavior recipe.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/tenant-culling.html", "title": "9.0.1.0 | Filters | Tenant Culling Filter", "content": "\nThe Tenant Culling filter is used on resources that need further specific tenant id information.\nThe filter looks at all the tenants that Keystone returns for a user to creates and populates a list of them based on the specific roles that were used to grant access to the resource.\n\nThe following filters are used to populate the Required Request Headers:\n\nThese headers prior values will be overwritten where existent.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nThis filter does not modify the response body.\n\nChanges to response headers vary based on configuration.\n\nX-Map-Roles is missing, can\u2019t be decoded, or is a malformed JSON object.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/translation.html", "title": "9.0.1.0 | Filters | Translation Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/uri-normalization.html", "title": "9.0.1.0 | Filters | URI Normalization Filter", "content": "\nThe URI Normalization Filter, as the name suggests, normalizes the URI of HTTP requests.\n\nNormalization is the process of standardizing content to optimize the flow of information.\n\nThis filter performs two primary functions: whitelisting and alphabetizing the query parameter list of the URI, and translating a media extension in the URI into an Accept header value.\nBoth of these functions may be useful in preventing cache busting.\n\nThis filter does not require any request headers.\n\nThis filter has no dependencies on other filters.\n\nIt is recommended that this filter be near the top of the filter chain so that the request is normalized before being processed by other filters.\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\n\nIt is recommended that this filter be near the top of the filter chain so that the request is normalized before being processed by other filters.\n\nThis filter does not modify the response body.\n\nThis filter does not create/modify any response headers.\n\nThis filter does not modify the response code.\n\nThis configuration will alphabetize query parameters by their key name.\nQuery parameters are expected to be key value pairs.\n\nThis configuration will whitelist certain query parameter keys.\nIf a query parameter key in the request does not match one of the whitelist query parameter keys, then the key and its associated value in the request will be removed from the request.\n\nThis configuration will add an Accept header to the request.\nIn some cases, if an Accept header is already present on the request, it will be replaced.\nNote that while the Accept header value may change, the body of the request will not.\n\nThe value of the Accept header is configurable.\nSince multiple values may be configured, this filter will select the most appropriate value.\nAt most one value will be selected.\nSelection follows an order of precedence:\n\nMedia type handling also has the effect of normalizing the Accept header.\nAs a result, it can be used to prevent cache busting that relies on varying the value of the Accept header.\n\nThis configuration will perform both media type normalization and query parameter normalization.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/uri-stripper.html", "title": "9.0.1.0 | Filters | URI Stripper Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/uri-user.html", "title": "9.0.1.0 | Filters | URI User Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nRemove this section if not needed.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/url-extractor-to-header.html", "title": "9.0.1.0 | Filters | URL Extractor to Header Filter", "content": "\nThis filter does not require any request headers.\n\nWhile this filter doesn\u2019t require any preceding filter, it\u2019s headers are added to any existing values.\nIf you desire to only get the new values you should use the header normalization filter.\n\nThis filter will create whatever headers you configure it to.\n\nThis filter does not make any changes to the request body.\n\nThere are no recommended follow on filters for this filter.\n\nThis filter has zero interactions with the response.\n\nThis filter has zero interactions with the response.\n\nThis filter has zero interactions with the response.\n\nThis configuration will exercise all the options available in this filter.\n\nIf a group is used to write a regular expression, but the value of that group should not be added to the header, a non-capturing group construct may be used.\nFor more information, see the special constructs portion of the Java Pattern documentation.\n\nThe regexes used in this filter are backed by a Java implementation.\nYou can learn more here.\n\nA regular expression is considered to be matching if it matches any substring of the request URL.\nA regular expression does not have to match the entire request URL to be considered matching.\nIf a regular expression should match the entire request URL, use boundary matchers (e.g., ^ and $).\nFor example, the regular expression ^/foo/bar$ will only be considered matching if it matches the entire request URL.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/valkyrie-authorization.html", "title": "9.0.1.0 | Filters | Valkyrie Authorization Filter", "content": "\nThe Valkyrie Authorization filter performs three distinct functions:\n\nCurrently, only JSON bodies are supported.\nIf you would like to request XML support, please contact us!\n\nEach function is optional, and as many or as few as desired can be used.\nAll functions involve making one or more call(s) to the Valkyrie service to ascertain a user\u2019s permissions.\nThe results of these calls are cached to cut down on the traffic generated by repeat requests.\n\nWhile there are no preceding filters that are strictly required, the following filters may be useful:\n\nThis filter does not modify the request body.\n\nThis filter is not strictly required by any other filters.\nHowever, the following filters may be useful:\n\nIf the collection-resources element is configured, this filter will perform culling on the response body.\nCulling is used to restrict the data transmitted to the user from the origin service.\nTechnically, culling is the removal of fields from the response body, and the updating of corresponding field counts.\nCurrently, only JSON content is supported.\n\nThis filter may also remove the response body if culling cannot be completed successfully.\nSee the [Response status codes] section for more details.\n\nFor more information about specific response codes that Repose will receive from the Valkyrie service, please refer to the Valkyrie documentataion.\n\nIf a user has the account_admin role in Valkyrie, when enable-bypass-account-admin is configured to be true, the Valkyrie filter will pass the request along regardless of whether or not the device permission check fails.\nCulling will also not be performed when configured in this manner.\nThe Valkyrie filter can add the user\u2019s permissions to the X-Roles header, but it is left to a subsequent filter or the origin service to validate the request.\n\nThis configuration will authorize users against Valkyrie.\n\nThis configuration will authorize non-admin users, translate permissions to roles, cull the response, and delegate any failures.\n\nThe enable-bypass-account-admin attribute applies to users with the role permission account_admin as well as requests with a X-Device-Id `header value containing a device ID to which the user has `account_admin device permissions.\nThis could unintentionally bypass culling.\nA X-Device-Id header should not be added or allowed on requests to endpoints where culling is performed.\n\nThis filter utilizes Keystone to authenticate with the Valkyrie service.\nThe X-Auth-Token header will be copied from the inbound request to Repose to the outbound request to the Valkyrie service.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/filters/versioning.html", "title": "9.0.1.0 | Filters | Versioning Filter", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\n???.\n\nThis configuration will ???.\n\nThis component reports the following metrics to the Metrics Service:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/api-event-flume.html", "title": "9.0.1.0 | Recipes | API Access Event Recording with Flume", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/basic-authentication.html", "title": "9.0.1.0 | Recipes | Basic Authentication", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/client-authentication.html", "title": "9.0.1.0 | Recipes | SSL/TLS Client Authentication", "content": "\nThis recipe explains how to utilize SSL/TLS Client Authentication in Repose.\n\nSSL/TLS provides an encrypted link between the client and the server, and is the standard for sending private data over the internet.\nYou may want to enable SSL/TLS Client Authentication if part of your infrastructure is in a different datacenter to ensure only cross service traffic can reach your service.\n\nSSL/TLS Client Authentication is being used more and more for communications between different enclaves.\nThis addition to the server based SSL/TLS handshake involves the Client presenting credentials to the Server in the same manner as the Server does to the Client.\nIf the credentials presented by the Client are not trusted, then the Server will sever the connection just as the Client would have if the situation was reversed.\nSince a Client initiates contact with the Server, the Server\u2019s credentials are simply to validate it is who the Client was trying to contact.\nThis is accomplished through Certificate Authorities (CA) and the Trust Hierarchies built into the Public Key Infrastructure (PKI).\nEven though you can optionally add a particular Server\u2019s credentials directly into a Client so that it will implicitly trust a particular Server essentially bypassing the distributed trust mechanism in favor of a more direct one, this is the only way to build a relationship for a Client to a Server.\n\nIn the following steps, the client will refer to the entity sending the request, and the server will refer to the entity receiving the request.\nIf you are setting up client authentication for requests inbound to Repose, then the entity sending the request is the client and Repose is the server.\nIf you are setting up client authentication for requests outbound from Repose to the origin service, then Repose is the client and the origin service is the server.\n\nUsing the JDK provided keytool generate the certificate that the client will use.\n\nAgain, using the keytool export the certificate.\n\nUsing the keytool import client certificate into the server keystore.\n\nWith your certificates in the right spot it\u2019s time to turn authentication on for Valve.\nThis is achieved with a simple tweak to the container.cfg.xml.\n\nTo turn on authenticated communication to your origin service, you\u2019ll need to tweak the default pool in your HTTP Client Service config.\nThere is nothing stopping you from similarly tweaking any of your other pools if you have a need to use client authentication in other outbound communication.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/custom-filter-for-repose.html", "title": "9.0.1.0 | Recipes | Custom Filter for Repose", "content": "\nUnfortunately, this page is not going to teach you how to create a custom filter for Repose.\nFortunately, this page is going to direct you to a page that will teach you how to create a custom filter for Repose.\n\nFor a detailed guide on how to create a custom filter for Repose, see our hello world filter project.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/delegation.html", "title": "9.0.1.0 | Recipes | Delegation", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/docker.html", "title": "9.0.1.0 | Recipes | Docker", "content": "\nWith the advent of container technology, Repose can be fully encapsulated and run as a service.\n\nBy bundling the environment with the software itself, deploying Repose has become a much quicker and simpler process.\nDocker boasts security through isolation, and when run on platforms with native support, it requires very little overhead.\nTo make everything as easy and versatile as possible, the Repose team now publishes Docker images on Docker Hub as part of every release.\n\nEven though the published Repose Docker image doesn\u2019t do a lot by default, this recipe will show you how to tap into some of that power hidden just below the surface.\n\nThe rest of this recipe assumes you have already completed the Quick Start and that you are familiar with how to perform the following on Repose Docker containers and images:\n\nIf you don\u2019t have the basics of these operations memorized, then that is fine.\nYou can always look back at the Quick Start anytime you need to.\n\nAll official Repose Docker images are published on Docker Hub at:\n\nIn the Quick Start the Repose instance was left in its default out of the box configuration.\nAs was seen, it wasn\u2019t very useful unless you wanted to go to the Rackspace homepage.\nSo you are probably wondering how you make a Repose container do cool stuff like Preventing XML bomb attacks.\n\nIf you already have a Repose configuration that you are migrating to a Docker container, then it is very easy to present it to the container.\nSimply add the location to the --volume option as described in the example below.\n\nIf you don\u2019t already have a configuration, then there is a couple of paths you might want to take.\nThe first is if you don\u2019t know how Repose works and therefore need to know what you can do, then visit the Getting Started.\nWe all need to get started somewhere and this is a great place if it is your first time using Repose.\nIf you are already familiar with using Repose and are just looking for a quick reference of the different files, the visit the Configuration page for more specific details.\nWe also have a series of Recipes that will walk you through some of our common use cases and their configurations.\n\nThe following command will mount a volume containing a configuration and perform a couple other new options to the run command which are explained below.\n\nLet\u2019s break that command down and take a closer look at what it is doing:\n\nNow if we wanted to, we could change the Docker options that define the environment that Repose will run in.\nIf we wanted to forward random ports instead of explicitly declaring the port mapping, we could replace the --publish 8080:8080 option with --publish-all instead.\n\nThe Dockerfile\u2019s used to build v{project-version} of the Repose Docker images published on Docker Hub are:\n\nTo build a custom Repose Docker image, follow these steps:\n\nLet\u2019s break that command down and take a closer look at what it is doing:\n\nCustom artifacts are not currently supported by our Docker images.\nIf you would like to deploy custom code in Repose running in Docker, please contact us!\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/echo-server.html", "title": "9.0.1.0 | Recipes | Echo Server", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/functional-test-framework.html", "title": "9.0.1.0 | Recipes | Functional Test Framework", "content": "\nThe Repose functional test framework is a testing library provided by the Repose team meant to simplify the process of creating functional tests for features in Repose or its extension components.\n\nThis framework is based on the Spock framework.\nWhile there are features of this framework that are can be used without Spock, it is expected that tests utilizing this framework will be Spock tests.\n\nThe Repose functional test framework is published to an instance of Nexus hosted by Rackspace.\n\nIt is highly recommended that a build system be used to manage this library.\nThe following snippets provide the necessary additions to download this framework with some popular build systems.\n\nThere are two requirements that must be satisfied for this framework.\n\nThe first requirement is that a repose-test.properties file is on the classpath when tests are run.\nThe repose-test.properties file tells that framework what version of Repose, directories, host, and ports to use.\nIt should be in the Java properties file format like the following example:\n\nNot all properties are strictly required.\nIn fact, most properties have reasonable defaults.\nHowever, any property which takes an absolute path should be given a value.\n\nThe second requirement is that the Repose artifacts (e.g., JARs and EARs) must be placed in the directory specified by the repose.home property.\nIf they are located elsewhere, the ReposeValveTest specification will not work.\nHowever, other utilities provided by the framework may still be used.\n\nTo make the most of this framework, tests should be written for the Spock testing framework.\nThe Repose functional test framework provides a base Spock Specification in the form of the ReposeValveTest class.\nThe ReposeValveTest specification provide utilities to populate configuration file templates, start Repose, and search the Repose log.\n\nThe following simple example demonstrates how a Spock test can be written utilizing the Repose functional test framework.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/getting-started.html", "title": "9.0.1.0 | Recipes | Getting Started", "content": "\nRepose is an open-source, RESTful middleware platform that transparently integrates with your existing infrastructure.\nTechnically, it is a pseudo-transparent reverse proxy.\n\nRepose provides highly scalable, extensible solutions to common API processing tasks including authentication, rate limiting, access control, logging, and more.\n\nAdditionally, Repose allows services to use Enterprise Integration Patterns.\n\nRepose takes incoming requests from clients and adjusts them for consumption by services by passing the requests through a series of extensible filters.\nThese filters provide functionality such as authentication, authorization, rate-limiting, and request modification.\nRepose can run as a stand-alone proxy server between the client and the origin service.\nThe Repose instance can be on the same host as the origin service, but it doesn\u2019t have to be.\nThis deployment method, illustrated in the following graphic, is called Valve.\n\nYou can decide which configuration works best for you, and you can tweak many of your configurations without having to restart Repose \u2013 it will pick up configuration changes on the fly, making it easy to configure and test.\n\nRepose can be configured to use a distributed data store service where cached information is exchanged across multiple nodes.\nThis makes Repose fault tolerant with proven performance.\n\nRepose provides a series of customizable filters that you can configure to perform a large number of API tasks.\nThe full list of filters is on the Filters page.\nFollowing are some of the most common ones:\n\nAnyone can use Repose!\nIt is an open-source platform for the general public to consume, share, and improve.\n\nTo get Repose, log in to your server and run the Installation commands for your desired environment.\nThen, configure your origin service endpoint in the system model, and you are ready to configure filters and services that work for you.\nWe have many sample configurations that you can use to get started.\nOr, because Repose is open source, you can build your own stacks of reusable software components.\n\nInstallation instructions are provided for Debian package managers (e.g., APT) and RPM package managers (e.g., Yum).\n\nCertain deployment methods will handle environment setup automatically.\n\nThe recommended and most common way to run Repose is as a standalone application using the Valve installation.\nThis installation allows Repose to be executed either as a Linux service (e.g., systemd) or directly from the installed JAR.\n\nThe most recent addition to the Repose family is a containerized release of the Valve installation.\nThis is great if you are looking to eventually run Repose with a container service or are just trying it out for the first time.\n\nWhile Repose can be installed manually, there are other deployment options available.\nHowever, these other methods do require a little more System Administration knowledge which is not covered here.\nSee: Deployment\n\nThis is the most common setup and is detailed on the Valve Installation page.\n\nAfter installing it this way, consult the Running Valve page for details on starting and stopping Repose.\n\nSee Quick Start for more details.\n\nOnce you know how you\u2019ll want to configure Repose, you may want to automate the deployment using a configuration management tool or reusable container.\n\nDocker and Repose make a fantastic team together.\nIf you are interested in using the published Repose Docker images, see our Quick Start.\nFor more details on building a custom Repose Docker image, see our Docker recipe.\n\nThere is an unsupported starter module in the GitHub repository rackerlabs/puppet-repose.\n\nThere is an unsupported starter cookbook in the GitHub repository rackerlabs/cookbook-repose.\n\nIf you\u2019re ready to dive into configuration, see Configuration for more details.\nWe also have a series of Recipes that will walk you through some of our common use cases.\n\nRepose has been battle tested in production environments and in our performance testing environments.\nSee Performance Best Practices for more details.\n\nFor further information on common configuration scenarios, visit our Recipes page.\n\nFor details on common troubleshooting techniques, visit our Troubleshooting page.\n\nFor a list of frequently asked questions and answers, visit our FAQ page.\n\nContact us! We would be happy to address any questions, comments, or concerns with anything having to do with Repose!\n\nIf you wonder what we\u2019ve been working on lately, visit our release notes.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/header-splitting.html", "title": "9.0.1.0 | Recipes | Header Splitting", "content": "\nPrior to Repose version 9.0, some request and response headers were split at the boundaries of Repose.\nThis recipe shows you how to configure Repose to split those headers, should you need to, in versions after the change.\n\nThe first thing to do is update your system model to include the split-header filter.\n\nAdd the new filter config to your config directory.\n\nThis list is exhaustive, it\u2019s recommended that you cut it down to the headers that you care about being split.\n\nIf you  have custom filters that need the exact behavior as previously provided, you should use two instances of the split-header filter.\nOne should be the request headers at the beginning of the chain, and one should be response headers at the end of the chain.\n\nYou can find more information on the split-header filter here.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/multi-tenant.html", "title": "9.0.1.0 | Recipes | Multi Tenant Authorization", "content": "\nThis recipe shows you how to configure your filters to support rest resources that contain multiple tenants in the uri\nall of which need authorization applied to them.\n\nFor the remainder of this recipe we are going to assume that you are trying to secure this resource: /a_tenant/do/something/on/another_tenant.\n\nThe first thing to do is to use the URL Extractor to Header filter to pull out both tenants we care about into a common header.\n\nFor authentication we need to configure a couple of options on Keystone v2 filter.\n\nFor basic tenant and endpoint authorization we use Keystone v2 Authorization filter.\n\nTo get per tenant based access control you need to configure the API Validator filter and the accompanying wadl.\n\nYou can find more info about configuring Role Based Access Control, here\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/performance-best-practices.html", "title": "9.0.1.0 | Recipes | Performance Best Practices", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/preventing-xml-bomb.html", "title": "9.0.1.0 | Recipes | Preventing XML bomb attacks", "content": "\nYou can configure the Translation filter a with a few different options to help prevent XML bomb attacks.\n\nSet allow-doc-type to true to allow the Translation Filter to accept substitutions within the limits of the secure processing feature of the Simple API for XML (SAX).\nThe current limitation of 100,000 is sufficient enough to be useful but limiting enough to not cause damage.\nAdditionally, this configuration will convert everything to UTF-8 which prevents attacks by content-type switching.\nWhen set to false, the filter will reject all doctype declarations.\nDefault: false\n\nControl the HTTP method verbs that are allowed by configuring the http-methods attribute.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/quick-start.html", "title": "9.0.1.0 | Recipes | Quick Start", "content": "\nTo get started in record time, we are going to deploy Repose using Docker!\n\nYou do not need to be a Docker expert to run Repose, but if you would like to learn more, check out the Docker Recipe and Official Docker Documentation!\n\nBefore you can use a Repose Docker image, you will need to install and have running a Docker Engine.\nThe Docker Engine is available for almost all operating systems.\nPlease follow the installation instructions for Docker Engine since that is outside the scope of this Quick Start guide.\nSince this is a Quick Start guide, only the minimal features required to get up and running are presented.\nFor a complete exploration of what the Docker Engine can provide, take a look at the Official Docker Documentation.\n\nAt this point you may be able to execute the docker run hello-world test.\nIf you can, then you are almost ready to run a basic Repose Docker image.\nIf you can\u2019t, then the steps below may get you up and going.\n\nThese are a few minor configuration items that may prevent some platforms from even executing the docker run hello-world test.\nThey are not always well documented in the installation instructions, but are easily remedied.\n\nYou will need to get a Docker Hub account.\nOnce you have that account, you can verify your credentials by logging in using:\n\nAnother issue that may arise is that communication with the Docker Engine is denied do to a lack of permissions.\nThis can be fixed by adding the user to the docker group.\nOn most *NIX systems this can be accomplished by issuing the following command as root or another elevated user (e.g., sudo):\n\nMake sure you can successfully run the Docker test:\n\nAll official Repose Docker images are published on Docker Hub at:\n\nAfter you have a running Docker Engine, you can run the latest Repose release with the following command:\n\nLet\u2019s break that command down and take a closer look at what it is doing:\n\nThe docker run command will return the full 64 character unique ID of the Container that was started.\nHowever, only the first 12 are typically needed for the CONTAINER_ID in the rest of the command examples.\nIf you no longer have the Container ID or you just want to see your handy work up to this point, then you can run:\n\nOnce running, you may wish to inspect the internal state of the container.\nWell, the default command for the official Repose Docker images will run Repose in the foreground, and as a result, simply attaching to the container with a command like the following will only allow interaction with the Repose process:\n\nTypically that is not very usefull.\nInstead, you can use a command like the one below to execute a new bash session inside of your running container:\n\nIn that shell you can check the current filter chain being used by the Repose instance with:\n\nTo end the interactive session, issue the exit command.\n\nThe expectations of Twelve-Factor App logging is that all internal logging mechanisms within the container be disabled.\nThe intent is that all traditional logging information should be streamed on STDOUT/STDERR.\nTo access a containers log information simply use the following:\n\nWhen the Repose instance in the Docker container is ready to accept and proxy requests, you will see a log message containing Repose ready.\n\nThe simplest way to connect to the Repose instance is using the cURL command:\n\nThe output will contain lines similar to these:\n\nIf you look at the container logs, then you will see a new entry under the Repose ready that contains Tracing header: {\u2026\u200b}.\nThis indicates that a new request was made to this instance and since you received the 301 Moved Permanently response, then you know the default configured Repose is working as expected.\n\nIf you connect to that same address using a regular web browser, you will automagically be redirected clear on through to the Rackspace homepage.\n\nOut of the box a default configured Repose doesn\u2019t do much.\nThis is because everybody needs it to do something specific to their needs.\nThere are a lot of different Filters available that can be used like building blocks to make a filter chain that does exactly what you need.\nOne of the things that Repose is used for is Preventing XML bomb attacks.\n\nIf you are ready to learn how to tweak the default configuration to do a little more or are feeling adventurous and want explore building your very own custom Repose Docker image, then head on over to the Docker recipe page.\n\nIf you have had your fill of running Repose in a local Docker container for right now, then you can stop it from running using:\n\nYou can always start it up again later with:\n\nIf you forget what the CONTAINER_ID is, then you can add the --all option to the ps command to see even the containers that aren\u2019t currently running.\n\nIf you have decided to completely give up on this Repose Docker container, then you can remove it with:\n\nIf you are never going to run another Repose Docker container, then you can remove the image used to create them with:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/rate-limiting.html", "title": "9.0.1.0 | Recipes | Rate Limiting", "content": "\nA common use case for Repose is rate limiting.\nIt limits how many requests per some unit of time (e.g., 10 requests per minute) are allowed to be made.\nIn this recipe, we\u2019ll be using the X-PP-User header to indicate who we are for rate limiting purposes.\nFor additional info on populating that header and on rate limiting by groups, see the Rate Limiting Filter documentation.\n\nThe filter can be enabled by adding it to the list of filters in the System Model.\n\nAfter the filter has been added to the System Model, the example configuration can copied/moved from the examples directory to the configuration files directory.\nThe example configuration for rate limiting is sufficient for testing and will limit requests to the origin service to 10 times per minute.\n\nYou can use this script to quickly make 11 requests to Repose to confirm that rate limiting is working.\n\nAssuming you named the script test_rate_limiting.sh, you can run it with the following:\n\nYou should see the first 10 attempts succeed with a 200 and the 11th attempt fail with a 413.\nA sample of the 11th attempt output is below.\n\nThis is real handy if you have a single Repose node, however if you scale your Repose cluster horizontally, then you will need to configure for Distributed Rate Limiting for it to behave as you would expect it to.\n\nIf no Distributed Datastores are available, then rate limiting will use the local datastore and each node will allow the configured rate through.\nThis is not typically the desired behavior and is easily remedied.\nBy default, rate limiting will be distributed using the standard Distributed Datastore (hash-ring) if it is available.\nHowever, the Distributed Datastore must be enabled in the System Model.\nFurthermore, any of the datastore types can be used to store rate limiting information.\nThe Distributed Datastore documentation has more information on how to properly enable and configure them.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/reason-phrase-normalization.html", "title": "9.0.1.0 | Recipes | Reason Phrase Normalization", "content": "\nThis recipe explains how to configure Repose to replace the reason phrase on the status line of an HTTP response with the reason phrases recommended by RFC 2616.\n\nPrior to Repose version 9.0, the reason phrase on the status line of the response from the origin service was not copied to the response returned by Repose.\nFollowing this recipe will, in effect, replicate that behavior by normalizing the reason phrase.\n\nUpdate your System Model to include the Scripting Filter.\n\nAdd the Scripting Filter configuration file to your configuration directory.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/role-based-access-control.html", "title": "9.0.1.0 | Recipes | Role Based Access Control", "content": "\nYou can implement Role-Based Access Control (RBAC) with Repose using the Authentication and Authorization mechanisms.\nThis guide takes you through the process of setting up RBAC with Repose.\n\nTo prevent users from submitting their own roles, you will need to blacklist headers using the Header Normalization filter.\n\nPlease refer to the Header Normalization filter documentation for more information about the available configuration options.\n\nThe Authentication filter will grab the user\u2019s roles from their authentication token and return those roles to Repose.\n\nPlease refer to the Keystone v2 filter documentation for more information about the available configuration options.\n\nThe Authorization filter will validate that the user has access to the tenanted resource being requested.\nIf the user is authorized to access a tenanted resource, the user\u2019s data in the request (e.g., the X-Roles, X-Tenant-Id, and X-Map-Roles headers) will be culled to those relevant to the requested resource.\nEndpoint validation can also be performed, if desired, but is outside the scope of RBAC.\n\nPlease refer to the Keystone v2 Authorization filter documentation for more information about the available configuration options.\n\nThere are are a few mechanisms for performing the enforcement.\nThe decision will come down to how complex your API is and the complexity of the authorization scheme.\n\nWe recommend that you build a table similar to the example below that contains endpoints and the roles that you wish to allow access to those endpoints.\n\nFrom here you have to decide which filter you want to use.\nIf your API is minimal or your just getting started with it, then you might find the Simple RBAC filter most useful.\nIt uses a very simple Domain Specific Language (DSL) that is similar to what other tools use for this basic mechanism.\nAnother powerful, but easy to configure tool is the RegEx RBAC filter.\nThis filter uses a similar configuration, but the resources are defined with Java Regular Expressions.\nIf on the other had your API is large and/or your authorizations are complex, then you will need the heavy lifting of the API Validator filter which uses a WADL to fully define the API.\nAs with anything, the more bells and whistles you need, the more complex the configuration will be.\n\nThe Simple RBAC filter is configured using a Domain Specific Language (DSL) similar to the table above.\n\nThe best part about this filter is that if your API grows or simply becomes to complex for the Simple RBAC filter, then you can easily move to the full API Validator filter later.\nThere is even a setting available to save your Simple RBAC filter configuration in a manner that you can use it immediately with the API Validator filter.\n\nPlease refer to the Simple RBAC filter documentation for more information about the available configuration options.\n\nThe RegEx RBAC filter is configured using a Domain Specific Language (DSL) similar to the one above.\n\nThis filter provides an RBAC mechanism for API\u2019s that don\u2019t conveniently fit into the restrictions imposed by the WADL specification.\n\nPlease refer to the RegEx RBAC filter documentation for more information about the available configuration options.\n\nIf your API is complex or you simply need or are already using some of the extra features available in the API Validator filter, then this is the choice for you.\n\nWhen the enable-rax-roles attribute for the API Validator filter is set to true, the check-headers attribute will also be enabled regardless of your setting.\n\nIn the WADL, include rax:roles with appropriate values to ensure access is controlled as expected.\nWhen defining rax:roles at the resource level, be aware that all sub-resources and methods will inherit the roles allowed at the resource level.\nMultiple roles can be specified by separating the role names with a space.\nIf multiple roles are authorized for a resource and method, the user must have one of the allowed roles but is not required to have all roles.\nThe following example shows a WADL for use with the API Validator filter that is configured for RBAC.\n\nWith the above WADL and API Validator filter configuration, the following behavior will apply for a request given the user has the a:observer role in the X-Roles header.\n\nPlease refer to the API Validator filter documentation for more information about the available configuration options.\n\nThe status codes returned by authorization failures, via rax:roles extensions (403), differs from the statuses returned when roles are defined directly in the validator.cfg.xml (404 and 405).\n\nIn the WADL, add to the rax:roles the name of appropriate values to ensure access is controlled as expected.\nAs always, rax:roles is a space separated list of names.\nAlso if they are defined at the resource level, all sub-resources and methods will inherit the roles allowed at the resource level.\nIf multiple roles are authorized for a resource and method, the user must have one of the allowed roles but is not required to have all roles.\nThe following example shows a WADL for use with the API Validator filter that is configured for Tenanted RBAC.\n\nWith the above WADL and API Validator filter configuration, the following behavior will apply for a request given the user has the a:observer role in the X-Map-Roles header for a tenant identified in X-Tenant-Id header.\n\nPlease refer to the API Validator filter documentation for more information about the available configuration options.\n\nThe case of the actual header does not matter, but the case of the rax:roles tenant must match the case of the param element\u2019s name attribute.\n\nThe difference between the regular RAX-Roles and the Tenanted RAX-Roles is subtle from a configuration standpoint, but they have very different expectations and outcomes.\nThe regular RAX-Roles uses the X-Roles header which is simply a plain text splitable header that contains all of the roles associated with the user.\nThe Tenanted RAX-Roles uses the X-Map-Roles header which is a base 64 encoded JSON map of strings to arrays of strings (e.g., the base 64 encoding of {\"someTenant\": [\"someRole\", \"sharedRole\"], \"otherTenant\": [\"otherRole\", \"sharedRole\"]}).\nThis allows for much more precise control over what access a user has to an API based on the current context (i.e. tenant) they are accessing it for.\n\nIf your origin service requires the X-Tenant-Id header to contain only the tenant id\u2019s pertinent to the RBAC Authorization roles that were provided in the X-Relevant-Roles header,\n    then enable the Tenant Culling filter.\n\nThe following example shows a basic System Model that enables the Tenant Culling filter.\n\nThere is no further configuration of this feature.\nSimply by including the Tenant Culling filter in the System Model after the Authentication and RBAC filters, it is enabled.\n\nEven though the Header Normalization filter isn\u2019t strictly required for tenant culling to work, it is a good idea to always include it before the first Authentication or Authorization filter.\n\nPlease refer to the Tenant Culling filter documentation for more information about this feature.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/running-valve.html", "title": "9.0.1.0 | Recipes | Running Valve", "content": "\nValve allows Repose to run as a standalone application.\nRepose currently supports multiple ways of starting and stopping in an effort to provide the most flexibility to operators during the industry wide conversion from System V and Upstart to systemd.\nThe SystemV and Upstart mechanisms are currently deprecated, and in the future, only support for the systemd way will be provided by the standard installation.\n\nThis is the recommended way of starting/stopping Repose.\nBy default, Repose will log to both the systemd logging system via standard out/err as well as system logs in /var/log/repose.\nThis behavior can be modified by updating the log4j2.xml in the Repose configuration directory to only log to one or the other depending on the need.\n\nThis is the old way of starting/stopping Repose.\n\nAlternatively, you could start/stop Repose using the init.d script directly.\n\nAs a Java application, Repose can also be run directly from a JAR file.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/user-access-events.html", "title": "9.0.1.0 | Recipes | User Access Events", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/valve-installation.html", "title": "9.0.1.0 | Recipes | Valve Installation", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\nCovers the typical Repose setup\u2009\u2014\u2009a Valve installation (i.e., standalone application) running as a Linux service.\n\nFor instructions on starting Repose after setup, see: Running Valve.\n\nFor more information about the packages themselves, see: Packages.\n\nNative packages are not yet provided for all operating systems.\n\nTo install Repose Valve on systems that do not support Repose packages, see: Manual\n\nThis installation method is not recommended unless necessary.\nThis method does not verify or modify the environment to accommodate Repose.\n\nIf you want to manually download the application archives, they are available in our Nexus Repository.\nThe primary archives are:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/recipes/xsd-versioning.html", "title": "9.0.1.0 | Recipes | XSD Versioning Guidelines", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/atom-feed-consumption.html", "title": "9.0.1.0 | Services | Atom Feed Consumption Service", "content": "\nThe Atom Feed Consumption Service is a service in Repose which enables simple reading of Atom feeds.\nBy centralizing this function in a service, Repose developers may share resources (and reduce overhead) when multiple Repose components need to read the same feed.\n\nThe Atom Feed Consumption Service can be enabled in a Repose deployment by adding it to the services list of the System Model like this:\n\nThis is the most basic configuration there is to a non-authenticating feed:\n\nThe legal values are:\n\nThe Atom Feed Consumption service is built on top of the Akka framework.\nIt utilizes the Akka actor system to scale horizontally as the number of Atom Feeds being monitored grows.\nIn addition, most processing done by the Atom Feed Consumption service is asynchronous and non-blocking.\n\nDue to the nature of the Akka actor system, if the Atom Feed Consumption Service is configured to poll a feed more quickly than that feed can be read, the Akka actor queue for that Feed will grow perpetually.\nThis will result in high memory usage, and eventually the JVM will run out of memory and crash.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/datastores.html", "title": "9.0.1.0 | Services | Datastores", "content": "\nRepose uses one of the following datastore implementations to store various types of data:\n\nThe local datastore is enabled by default.\nThe others are enabled by including the appropriate service into your System Model configuration.\n\nIf no other datastores are configured, then Repose will use the local datastore.\nThe local datastore will store data using the cache on each node.\nData will not be shared among the nodes, so each Repose node will have its own copy of the data.\nFor example, if using rate limiting with the local datastore, then each node will track its own limit information and limit updates will not be shared with other nodes.\n\nA Repose node may, at times, need to share information with other Repose nodes.\nThe Distributed Datastore component allows Repose to host a simple hash-ring object store that shares consistency between all of the participating nodes.\nThis, in turn, allows other hosted components as well as external programs to use a cluster of Repose nodes as a whole to store information.\nInstead of cache operations communicating through the Repose Service port (which is the port a Repose instance services requests to pass to the Origin Service), the Distributed Datastore Service will communicate through configured port(s) within the distributed datastore configuration.\nIf the Distributed Datastore Service is unable to communicate with the service on other nodes, it will fall back on the local datastore temporarily.\nOnce other nodes become reachable, the Distributed Datastore Service will return to being distributed.\n\nThe Distributed Datastore service can be enabled in a Repose deployment by adding it to the services list of the System Model like this:\n\nAdding the Distributed Datastore Service to a Repose deployment requires that listening ports be configured within the dist-datastore.cfg.xml file.\nThe <port> element is the port configuration for the Distributed Datastore.\nWhen you configure Repose to start with the Distributed Datastore, the running Repose instance will try to find the <port> configuration that matches it\u2019s own node ID in the System Model.\nIf the node attribute is not defined for a <port>, that <port> is considered the default <port>.\nA running Repose instance will listen on the default <port> for Distributed Datastore traffic if no <port> is defined with a node attribute matching its own node ID in the System Model.\n\nThe following is a basic sample configuration.\n\nThe distributed datastore provides the option to encrypt communication between nodes using HTTP over SSL/TLS.\nAs mentioned above, this is achieved by configuring a keystore in the dist-datastore.cfg.xml file.\nThis feature can additionally provide client authentication during the SSL handshake.\nBy both validating the client credentials and encrypting communication with the client, data in the datastore is made more secure.\n\nAssuming all Repose nodes are configured identically, the most straight-forward way to make use of this security would be to use a single unique keystore as both the keystore and the truststore.\nThis can be achieved by not explicitly configuring a separate truststore.\nSince each datastore node will have a copy of the keystore, each node will trust every other node.\n\nClient authentication in SSL/TLS can act as as alternate form of client validation, performing a task similar to that of an access control list.\nAs such, the usage of client authentication may replace the need to configure the allowed-hosts section of the dist-datastore.cfg.xml file.\n\nThe distributed datastore will use a connection pool from the HTTP Client Service to communicate across nodes.\nIf a connection pool is not configured, the default will be used.\nIn nearly all cases, the connection pool being used should not be the default, but rather, a connection pool should be configured to use SSL/TLS Client Authentication configured in the distributed datastore.\nThat is, the distributed datastore may be thought of as a server, and clients in the connection pool as clients.\nBoth the client and server need to be aware of how to communicate, and so they both must be configured with the appropriate secrets.\n\nFor managing keystores and truststores, the aptly named keytool can be used.\n\nFor more details, see:\n\nThe distributed datastore shares key-space with all of the enabled cluster nodes.\nKey-space is determined by the maximum value of the distributed datastore\u2019s hashing algorithm.\nCurrently the only supported hashing algorithm is MD5.\n\nAddressing a key is done by first normalizing all of the participating cluster nodes.\nThis is done by an ascending sort.\nAfter the participating nodes have had their order normalized, the key-space is sliced up by dividing the maximum possible number of addresses by the total number of participating nodes.\nThe given key is then reduced to its numeric representation and a cluster node is looked up by performing a modulus such that (<key-value> % <number-of-cluster-members>).\n\nBy default, the internal Repose client implementation for the distributed datastore will obscure key-space by storing only the MD5 hash value of a given key and not the key\u2019s actual value.\nThis is important to note since external gets against the distributed datastore must be aware of this functionality.\nThe MD5 hash is represented as a 128bit UUID.\n\nExample Key Addressing\n\nIf an external application makes a request for data stored by Repose components, it must first hash the key using MD5 before sending the request such that\u2026\u200b\n\nGET /powerapi/dist-datastore/objects/object-key\n\nbecomes\n\nGET /powerapi/dist-datastore/objects/cecda330-5a61-26cd-1a71-d5fe34a8e302\n\nObscuring key-space is not a function of the distributed datastore service.\nThis functionality is only present in the internally consumed java cache client.\nIf an external application puts an object into the distributed datastore, the object will be stored under the value of the key given.\n\nThe repose distributed datastore component is a service that hosts a simple RESTful API that can be contacted to perform remote object store operations.\nThese operations are defined below.\n\nGET /powerapi/dist-datastore/objects/<object-key> HTTP/1.1\n\nGets a stored object from the datastore by its key.\n\nPUT /powerapi/dist-datastore/objects/<object-key> HTTP/1.1\n\nPuts an object into the datastore by its key.\n\nDELETE /powerapi/dist-datastore/objects/<object-key> HTTP/1.1\n\nDeletes an object from the datastore by its key.\n\nPATCH /powerapi/dist-datastore/objects/<object-key> HTTP/1.1\n\nPatches an object in the datastore by its key.\nIf the object does not exist, then a new one created.\nReturn the modified/new object.\nThe object must be Patchable.\n\nIn the event that a node with in a datastore cluster falls off line or is unable to respond to requests, it is removed from the node\u2019s cluster membership for a period of time.\nDuring this time, the online node will then re-address its key-space in order to continue operation.\nAfter certain periods of rest, the node may attempt to introduce the damaged cluster member into its cluster membership.\nA damaged cluster member must go through several validation passes where the member is introduced back into the addressing algorithm before it can be considered online.\nIn order to keep healthy nodes from attempting to route requests to the damaged node, a participating node may tell it\u2019s destination that the destination may not route the request and must handle the value locally.\n\nThe Repose node will open sockets each time it has to communicate with other Repose nodes to share information.\nDuring times of load this can affect performance and data integrity as when one node cannot communicate with another it will mark that node damaged and store/create information locally.\nOne way this can happen is if the user running repose hits their open file limit.\nLuckily this can be mitigated by increasing the open file limit for the user running Repose.\n\nCurrently Repose instances do not report Distributed Datastore information to JMX.\nThis is something that has been done in the past, but an upgrade to the metrics library used has made this capability incompatible with the current codebase.\n\nAt times, Repose instances may need to share information between nodes that are unaware of each other.\nAn example of this is a dynamic containerized environment like OpenShift or other 12-Factor environment.\n\nThe Remote Datastore Service allows dynamic isolated Repose instances to use a single static Repose instance\u2019s object store.\nThe Remote Datastore Service communicates to the configured host through the configured port.\nIf the Remote Datastore Service is unable to communicate with the configured object store, it will fall back on the local datastore temporarily.\nThe Remote Datastore Service will return to using the configured object store as soon as it becomes reachable again.\n\nThe static Repose instance is simply configured as a single node cluster with the distributed datastore service enabled.\nThe distributed datastore is typically configured with SSL/TLS Client Authentication so that all clients must properly authenticate before a session is established.\nThen all of the dynamic clients are configured with a remote datastore and the HTTP Client Service configured to provide the proper authentication.\n\nThe Remote Datastore service can be enabled in the dynamic clients by adding it to the services list of the System Model like this:\n\nThe following is a basic sample configuration.\n\nRefer to the HTTP Client Service and SSL/TLS Client Authentication documentation for properly securing the connection pool used to connect to the Remote Datastore.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/health-check.html", "title": "9.0.1.0 | Services | Health Check Service", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/http-client.html", "title": "9.0.1.0 | Services | HTTP Client Service", "content": "\nThe HTTP Client Service provides a centralized way of managing and reusing HTTP clients for outbound communication.\n\nThis service introduces the concept of named connection pools.\nA named connection pool is described by a number of configuration options that dictate how a connection from that pool should behave.\nConnections from a pool are reused to cut down on the overhead of connection negotiation when resources are frequently accessed.\nThe id/name of the pool corresponds to the connection pool id found in other services' and filters' configurations, indicating that outbound connections from that component should use the matching pool.\n\nWhile named connection pools are the abstraction presented for configuration, HTTP clients are the abstraction presented for use in code.\nIn reality, an HTTP client manages a connection pool.\nA developer leveraging this service will interact with the HTTP client abstraction.\nAn operator configuring this service will interact with the named connection pool abstraction.\n\nIf the connection pool is filling up, but more resources are available on the machine Repose is running on, the pool can be expanded to allow more concurrent connections.\nThis is most useful if wait times are high or requests are timing out.\nThe size of the pools can be increased by raising http.conn-manager.max-total and http.conn-manager.max-per-route.\n\nSome services require a static header be present just for the purposes of identification; this can easily be done by adding the headers element to the pool configuration.\n\nIn certain environments, it may be necessary to utilize a proxy for outbound requests.\nFor example, Repose running on a private network may not be allowed to make direct connections to resources outside of the network.\nInstead, controlled connections can be made with a proxy which will forward requests to their intended destinations.\n\nTo configure the usage of a proxy, simply update the configuration for this service as done below.\n\nSee SSL/TLS Client Authentication\n\nFor more information about the http.* attributes or the underlying connection pool see Apache Connection Management.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/http-logging.html", "title": "9.0.1.0 | Services | HTTP Logging Service", "content": "\nThe HTTP Logging Service enables logging of operator-configurable messages for every HTTP interaction.\n\nUsage of this service is just a matter of configuring messages.\nMessages tell this service what to log and where to log it.\nMessages include a template to define what should be written to a log.\nTemplates will be populated and rendered using data relevant to each HTTP interaction.\n\nThis service can be used for auditing all interactions with an API.\n\nThis configuration sets every available setting explicitly for the sake of detailing the available settings.\n\nThis configuration includes a content template that conforms to the Common Log Format.\nThis template shows how some variables can be accessed, and makes use of some JTwig functions.\n\nThis configuration includes a content template that contains much of the same data as the Common Log Format (CLF) formatted in JSON.\nRather than rendering - for an undefined value, this template uses JTwig\u2019s conditional control flow mechanism to render null.\nIf desired, the same mechanism could be used to omit a field rather than rendering a null value for that field.\n\nIn order for this service to adapt to a myriad of potential use-cases, content templates are supported.\nThese templates are rendered using the JTwig templating engine.\nFor details on how to write a JTwig template, please refer to the JTwig reference documentation.\n\nFor compatibility reasons, this service uses different delimiters than JTwig for control flow code islands.\nThe following table shows the difference:\n\nThe following table details every variable available for use in a content template.\n\nThe headers on the inbound request.\nThe keys in this map are the header names while the values are lists of corresponding header values.\nThe keys (i.e., header names) are case-sensitive and will only contain lowercase letters.\nHeaders with multiple values spanning multiple lines will have a value list with one item for each line.\nFor example, given the header lines:\n\nThe value of this variable would be:\n\nThe headers on the outbound request.\nThe keys in this map are the header names while the values are lists of corresponding header values.\nThe keys (i.e., header names) are case-sensitive and will only contain lowercase letters.\nHeaders with multiple values spanning multiple lines will have a value list with one item for each line.\nFor example, given the header lines:\n\nThe value of this variable would be:\n\nThe headers on the outbound response.\nThe keys in this map are the header names while the values are lists of corresponding header values.\nThe keys (i.e., header names) are case-sensitive and will only contain lowercase letters.\nHeaders with multiple values spanning multiple lines will have a value list with one item for each line.\nFor example, given the header lines:\n\nThe value of this variable would be:\n\nIf a the value of a variable cannot be resolved, the variable will be left undefined.\nWhen rendering a template, an undefined variable has specific semantics which are detailed in the JTwig reference documentation.\n\nA distinction is made between inbound and outbound requests and responses.\nThis distinction allows template authors to retrieve data associated with a request or response at a particular point during the processing of the HTTP interaction.\nThe inbound qualifier denotes the point at which an HTTP message is received by Repose.\nThe outbound qualifier denotes the point at which an HTTP message is sent by Repose.\nAn inbound request and an outbound response define the interaction with an end-user while an outbound request and inbound response define the interaction with the origin service.\n\nWhitespace in templates is preserved.\nHowever, it is possible to modify whitespace when rendering a template by leveraging Jtwig whitespace control and the Jtwig trim function.\nWhitespace can also be modified when a log message is being written using the Log4J2 Pattern Layout replace pattern.\nThose mechanisms should help in cases where it is desirable to include whitespace for configuration formatting that is not desired in the rendered message.\n\nThis service introduces the concept of an HTTP logging context.\nAn HTTP logging context is a container for data related to an individual HTTP interaction.\nAn HTTP interaction is the exchange of an HTTP request and response.\n\nWhile this service provides the means to open and close a logging context, it does not do so automatically.\nLogging contexts are opened and closed by code which uses this service and is hooked into the container lifecycle.\nWhile it is possible to create a new logging context at any time using this service, it is not the expected usage.\nRather, the logging context created when a new request is received is expected to be used.\nThat logging context will be passed around in a request attribute with the key org.openrepose.core.services.httplogging.context.\n\nTo be clear, this service only manages:\n\nNotably, this service does not manage:\n\nManagement of those facets is left to developers intent on interacting with the logging system supported by this service.\n\nThere may be instances when it is desired to add data to the logging context that is not part of the predefined data set.\nFor this reason, the logging context supports extension data.\nExtension data can be accessed and modified via the extensions field of the logging context.\nExtension data is not validated by this service; it is passed as a map directly to the templating engine.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/logging.html", "title": "9.0.1.0 | Services | Logging Service", "content": "\nThe Logging service configures the logging library used by Repose.\n\nConfiguration for the logging library is provided by the user.\nConfiguration allows the user to decide what to log and where to log it.\n\nThe Repose logging system is currently backed by Log4j2.\n\nFor information on the syntax and semantics of the log4j2.xml file, please see the official Log4j2 configuration documentation.\n\nLog messages that are written before this service has initialized (e.g., Valve logs, core service startup logs) may not appear in the expected, configured log file.\nSince this service performs logging configuration, logging that occurs before it has initialized will rely on the Log4j2 default configuration which writes everything to standard streams (i.e., stdout, stderr).\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/metrics.html", "title": "9.0.1.0 | Services | Metrics Service", "content": "\nThe Metrics Service enables the registration, collection, and reporting of metrics across Repose.\n\nMetrics collected by this service provide insight into the state of Repose and its components at any given time.\n\nThe goal of the Metrics Service is to provide a simple, convenient mechanism for gathering and reporting metrics.\nTo that end, the Metrics Service manages a centralized metrics registry and reporters for that registry.\nBy exposing the Metrics Service as a Java @Named component, other Repose components may easily leverage the ability to record metrics.\n\nImplementation wise, the Metrics Service is a light-weight wrapper around the Dropwizard metrics library.\nThis allows the service to offer all of the capabilities of a powerful, open-source metrics library.\n\nFor more information about what the Metrics service can do, see the Dropwizard Metrics User Manual.\n\nAll metrics reported to this service will be reported to JMX.\n\nIn order to report metrics to JMX, this service must translate metric names into JMX ObjectNames.\nA metric can be named virtually anything, but names often follow the <segment1>.<segment2>.<\u2026\u200b>.<segmentn> convention.\nA simplified general form for an object name is <domain>:<key1>=<value1>,<key2>=<value2>,<\u2026\u200b>,<keyn>=<valuen>.\nTo go from metric name to object name, the translation process will start by prepending the metric name with the hostname of the host running Repose.\nNext, it will split the metric name using . as a delimiter.\nThe index of each delimited segment will act as the key for that segment, and the value of the segment will act as the value.\n\nFor example, a metric named org.openrepose.example.count will be reported to JMX as <hostname>:001=\"org\",002=\"openrepose\",003=\"example\",004=\"count\".\n\nWhile this approach can cause some difficulty in querying metrics from JMX, it provides easier navigation of metrics when using a tool such as JConsole.\n\nThis service supports the automatic aggregation of certain metrics.\nIn some cases, aggregating metrics provides useful insight into the system at a level where individual metrics may not be present.\nFor example, a Meter on its own might track the status codes sent in responses from a single filter.\nHowever, we want to be able to view the status codes send in responses from all filters.\nFor that purpose, this service supplies a SummingMeter.\nSee [SummingMeter] for more details.\n\nThe following nested sections provide details about the supported aggregation metrics.\n\nMultiMeter s can be used to mark multiple Meter s at the same time.\nWhile this makes MultiMeter s generally useful, when used in conjunction with the SummingMeterFactory, they are used to track an additional Meter which serves as the sum of all Meter s created by the SummingMeterFactory.\n\nA summing Meter should be constructed by utilizing the SummingMeterFactory accessible via this service.\n\nThe SummingMeterFactory also provides support for Meter trees.\nThese trees enable nesting of Meter s in more interesting ways.\n\nThe following lists attempt to aggregate all of the metrics being reported to this service by various components.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/open-tracing.html", "title": "9.0.1.0 | Services | OpenTracing Service", "content": "\nThe OpenTracing service, which adheres to the OpenTracing standard, enables reporting of tracing data to a collection service.\n\nThe initial release of the OpenTracing service only supports using the Jaeger client library.\nHowever, other Tracers can be integrated in future OpenTracing service updates.\nBy default, the service is disabled, but it can be enabled by including an open-tracing.cfg.xml configuration.\nThe origin service can simply pass the Tracer-specific header to downstream services and requests will be traced across application boundaries.\n\nFor backwards compatibility with the underlying configuration mechanism, the only way to disable the OpenTracing service after it has been started is to restart the Respose instance.\n\nThis section will cover all Tracers that are supported by the OpenTracing service.\nWe will be adding to this list as more tracers are supported.\n\nCurrent support:\n\nThis configuration will send all traces to the collector at http://localhost:14268.\n\nThis configuration will send one of every 1,000 traces to the collector using Basic HTTP Authentication.\n\nThe username attribute must be accompanied by a password attribute.\nThe token attribute is mutually exclusive to the username/password attributes.\n\nThis configuration will send at most one trace per second to the collector using UDP.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/phone-home.html", "title": "9.0.1.0 | Services | Phone Home Service", "content": "\nThe Phone Home service (a.k.a, Telemetry Service, Usage Statistics Service) is a service in Repose which will send usage data to a collection service.\nUsage data consists of the filters in use, the services in use, contact information, and other information relating to a specific instance of Repose.\nWhen enabled, this data is sent to a collection service owned and managed by the Repose team.\nThis data is used to improve Repose as a product by providing insight into common usage patterns and metrics.\n\nThe Phone Home service is activated by adding a phone-home element to the system model configuration file and setting the enabled attribute to true.\nThis element is included in the default Repose installation, but is disabled.\nExplicit action must be taken for the Phone Home service to be used.\n\nWhen the Phone Home service is disabled, or when it cannot successfully send an update message to the configured data collection service, the content of the update that would have been sent will be logged to a logger named phone-home-message.\nA user may choose for this logger to append to a separate file, or to simply append to the regular log file.\nIf the output of this logger is captured somewhere, a user may manually transmit the message generated by the Phone Home service wherever the user desires.\nNote that using this logger provides an easy way to inspect exactly what the Phone Home service is sending over the wire to the data collection service.\nAn example logging configuration where Phone Home messages are logged to a file is shown below:\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/services/uri-redaction.html", "title": "9.0.1.0 | Services | URI Redaction service", "content": "\nThe URI Redaction service enables Repose to redact sensitive information contained in a URI.\n\nThis is extremely useful for removing user tokens from API calls prior to creating OpenTracing spans.\nThe URI Redaction service can be used by any service or filter, even custom ones, that need to redact any information from a URI during processing.\n\nThe configuration for this service is extremely simple.\nIt consists of a root element and a repeating list of a single element.\nThe values of the repeating redact elements are regular expressions (RegEx\u2019s) that need to adhere to the Java Regular Expression syntax.\nEach of the RegEx\u2019s will be processed in order from top to bottom.\nThe first RegEx will be passed the original URI.\nThen the result passed to the next RegEx and so on through the list.\nEach capture group will be replaced with the literal XXXXX.\nIf multiple capture groups are present in a single RegEx, then all of them are redacted.\n\nNested capture groups are not supported and should not be used.\n\nSometimes it is easier to replace some or all of a RegEx with XML Entities that will be expanded during processing.\nThis can provide a more readable and therefore maintainable configuration.\n\nSometimes it is easier to replace some or all of a RegEx with XML Entities that will be expanded during processing.\nThis can provide a more readable and therefore maintainable configuration.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/welcome/faq.html", "title": "9.0.1.0 | Welcome to Repose | FAQ", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\nThese are frequently asked questions.\nIf you don\u2019t find an answer to your question here, check out the contact us section for information on getting in touch with the team.\n\nOnly the following HTTP methods are supported by Repose.\n\nRequests with any other HTTP method will be rejected with a response code of 400.\n\nPlease read our Troubleshooting page.\n\nIN PROGRESS\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/welcome/release-notes.html", "title": "9.0.1.0 | Welcome to Repose | Release Notes", "content": "\nUpgrade Notes\n\nThe case of the actual header does not matter, but the case of the rax:roles tenant must match the case of the param element\u2019s name attribute in the API Validator filter's WADL.\n\nMulti-Tenant support is currently not supported when the mask rax roles feature is enabled.\n\nThe org.openrepose.powerfilter.PowerFilter.trace-id-logging Logger in your Log4j2 configuration will determine the logging behavior for trace ID logging.\nIf the org.openrepose.powerfilter.PowerFilter.trace-id-logging Logger has not been configured, it will inherit the org.openrepose.powerfilter.PowerFilter logger\u2019s configuration.\n\nAs part of this correction, any configurations that were taking advantage of this lack of validation will cease to function.\n\nThis message is logged to a separate logger and can be disabled by adding the following to the log4j2.xml:\n\nAs part of the upgrade, some metric names reported by various components have been changed.\nFurthermore, all metrics reported to JMX via the Metrics Service now follow a new naming scheme.\nDue to a technical issue with the new version of the metric library, EHCache metrics are no longer being reported, but there is planned work to restore them.\nSee Metrics Service for details on the metrics currently being reported.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/welcome/style-guide.html", "title": "9.0.1.0 | Welcome to Repose | Style Guide", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\nDon\u2019t wrap text at a fixed column width.\nInstead, put each sentence on its own line, a technique called sentence per line.\nThis technique is similar to how you write and organize source code.\nThe result can be spectacular.\n\nHere are some of the advantages of using the sentence per line style:\n\nWe picked up this idea from the writing guide in the Neo4j documentation.\nHowever, it seems like the idea dates back a discovery by Buckminster Fuller in the 1930s, who called it ventilated prose.\nThe technique was also recommended in 2009 by Brandon Rhodes in a blog post about Semantic Linefeeds.\n\nIt\u2019s important to note that this technique works because AsciiDoc doesn\u2019t treat wrapped lines in prose as hard line breaks.\nAt least, it doesn\u2019t show up that way to the reader.\nThe line breaks between contiguous lines of prose will not be visible in the rendered document (i.e., as the reader sees it).\n\nWhile line breaks don\u2019t appear in the output, a blank line introduces a new paragraph.\nUse paragraphs to structure your text and don\u2019t make them too large.\n\nSection titles should be defined using Atx-style.\n\nIn the Atx-style, the section title is defined on a single line.\nThe section title begins with one or more equal characters (i.e., =) followed by a space and the section title.\nThe number of leading characters representing the depth.\nThe top-level section is reserved for the document title, so the first section in the document will have two leading characters.\n\nOf the supported section title styles, this style requires the least number of characters, and it\u2019s intuitive since the number of leading characters represents the heading level.\nIn addition, section titles, other than the top-level document title, should not have any blank lines directly below them before a sub-section title or the content.\n\nUse * to define lists rather than -.\nNesting in lists will correspond to the number of leading asterisks.\n\nThe name of the Repose project should be bold (i.e., surrounded with asterisks (*)).\n\nIf Repose is used as part of a full product name (e.g., Repose Docker image), then the proper names are all bold, but the common item is not.\n\nThe abbreviations i.e. and e.g. can be used if the list following them is complete or incomplete respectively.\nThey both must be followed by a comma.\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/welcome/troubleshooting.html", "title": "9.0.1.0 | Welcome to Repose | Troubleshooting", "content": "\nThis version of the docs is a work in progress.\nIf you don\u2019t see what you are looking for check the legacy wiki.\n\nThis page summarizes the different methods which can be used to get more information on the state of Repose and how it interacts with the clients, origin service, and third party services.\nMost of these methods can be used in a production deployment.\n\nRepose logs contain a great deal of information about the state of Repose, and as such, is usually a good place to start when troubleshooting!\nMost errors that occur in Repose will have an associated error message in the Repose logs.\nOften, these error messages are accompanied by a stacktrace detailing exactly what happened.\n\nThe configuration of the logging system has a substantial impact on the quality of logs.\nCertain configurations may even prevent useful log messages from being written.\nPlease see Logging Service for more details.\n\nLog messages that are written before the Logging Service has initialized (e.g., Valve logs, core service startup logs) may not appear in the expected, configured log file.\nSince the Logging Service performs logging configuration, logging that occurs before it has initialized will rely on the Log4j2 default configuration which writes everything to standard streams (i.e., stdout, stderr).\n\nThis should be used in a controlled environment (i.e., not production) and should not be used for more than a few minutes at a time due to the amount of logging that occurs.\n\nIntrafilter Logging allows you to see the modified output of each filter as the request goes through the filter chain which makes debugging configurations easier.\nThis setting adds a UUID header to the request and another to the response.\n\nWhen the logger is set to TRACE, you will see the following information:\n\nThe TRACE setting is the most verbose logging setting.\n\nTo enable intrafilter logging, the corresponding named log4j2 logger should be set to the TRACE level.\nThis can be achieved by adding a line like the following to the log4j2.xml file in your Repose configuration directory.\n\nAdditionally, the X-Trace-Request header must be present on the HTTP request.\nOnly requests with the X-Trace-Request header are eligible to have intrafilter logging enabled for them.\nThis constraint provides per-request intrafilter logging, which is helpful in limiting the scope and magnitude of data in the logs making it easier for the logs to be analyzed.\n\nThis is an example of the log line that you will see for the request and for the response.\nThese lines are formatted for convenience.\nThe log line will be a single line without carriage returns.\n\nRepose can report on which filters handled the client request and how long each filter took.\nThis allows you to verify that all expected filters handled the request.\n\nTo enable this, add the following header to the client request:\nx-trace-request: true\n\nTechnically, the value of the x-trace-request header does not matter at this time, but by convention, we pass a value of true.\n\nWhen that header is present on the request, Repose will add state to the logging context for the request that enables per-request tracing logging based on the Log4j configuration.\nThe filter-timing logger is used to log the time each filter takes to process.\nEach filter will have its own log message.\n\nTo configure Log4j to filter log messages based on the the content of the logging context, the CompositeFilter, ContextMapFilter, and ThresholdFilter can be used:\n\nThe x-trace-request header can be easily added to a test request using curl and the -H [header] flag.\nFor example:\n\nThe reported time is inclusive of the processing time of all succeeding filters in the filter chain and the origin service.\n\nThe following example contains three headers which Repose added to the response.\nThese headers indicate that the the client-auth, http-logging, and default-router all handled the request, taking 0, 6, and 0 ms respectively.\n\nYou can view all of the data being sent and received to the origin service and other third party APIs (e.g., Identity) by turning on Apache HttpClient logging.\n\nIn log4j2.xml, update the org.apache logger to the debug level.\nFor example:\n\nRepose instruments its internal operations, such as the types of response codes returned from Repose or the origin service and the number of active requests.\nThis information is published through JMX and can be accessed through any JMX client.\nJConsole is a popular choice to access information published through JMX as it is shipped with the JDK.\n\nIN PROGRESS\n", "site_name": "http://openrepose.org/versions/"}, {"topic_url": "9.0.1.0/welcome/ver-9-upgrade-notes.html", "title": "9.0.1.0 | Welcome to Repose | Version 9 Upgrade Notes", "content": "\nThen you would configure the URL Extractor to Header Filter with:\n", "site_name": "http://openrepose.org/versions/"}]